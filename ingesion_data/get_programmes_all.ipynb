{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a25700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libs\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import pathlib\n",
    "from urllib.parse import unquote\n",
    "\n",
    "# third party\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "# selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# folders\n",
    "BASE_DIR = pathlib.Path().resolve()\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# start pages\n",
    "LISTING_URL = \"https://studiegids.vu.nl/en/bachelor/2025-2026#/\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# selenium driver\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless=new\")  # uncomment for headless runs\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "wait = WebDriverWait(driver, 25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dd3ee3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q(css, timeout=25):\n",
    "    return WebDriverWait(driver, timeout).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, css))\n",
    "    )\n",
    "\n",
    "def q_all(css, timeout=25):\n",
    "    WebDriverWait(driver, timeout).until(\n",
    "        EC.presence_of_all_elements_located((By.CSS_SELECTOR, css))\n",
    "    )\n",
    "    return driver.find_elements(By.CSS_SELECTOR, css)\n",
    "\n",
    "def click_el(el):\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", el)\n",
    "    time.sleep(0.2)\n",
    "    el.click()\n",
    "\n",
    "def try_click(css, timeout=3):\n",
    "    try:\n",
    "        el = WebDriverWait(driver, timeout).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, css))\n",
    "        )\n",
    "        click_el(el)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def dismiss_cookies():\n",
    "    def click_buttons():\n",
    "        xpaths = [\n",
    "            \"//button[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ','abcdefghijklmnopqrstuvwxyz'),'accept')]\",\n",
    "            \"//button[contains(., 'Akkoord')]\",\n",
    "            \"//button[contains(., 'Alles accepteren')]\",\n",
    "            \"//button[contains(., 'Accept all')]\",\n",
    "            \"//button[contains(., 'Accept')]\",\n",
    "        ]\n",
    "        for xp in xpaths:\n",
    "            try:\n",
    "                btn = WebDriverWait(driver, 2).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, xp))\n",
    "                )\n",
    "                click_el(btn)\n",
    "                return True\n",
    "            except Exception:\n",
    "                pass\n",
    "        return False\n",
    "\n",
    "    if click_buttons():\n",
    "        return\n",
    "    frames = driver.find_elements(By.CSS_SELECTOR, \"iframe\")\n",
    "    for fr in frames:\n",
    "        try:\n",
    "            driver.switch_to.frame(fr)\n",
    "            if click_buttons():\n",
    "                driver.switch_to.default_content()\n",
    "                return\n",
    "        except Exception:\n",
    "            driver.switch_to.default_content()\n",
    "        finally:\n",
    "            driver.switch_to.default_content()\n",
    "\n",
    "\n",
    "# helper to detect Minor sections\n",
    "def is_minor_text(txt):\n",
    "    return bool(re.search(r\"\\bminor\\b\", (txt or \"\"), flags=re.I))\n",
    "\n",
    "def safe_text(el, sel=\".accordion-title\"):\n",
    "    try:\n",
    "        t = el.find_element(By.CSS_SELECTOR, sel).text.strip()\n",
    "        if t:\n",
    "            return t\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return el.text.strip()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32c1ce8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Programmes found: 17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Ancient Studies',\n",
       " 'Archaeology',\n",
       " 'Artificial Intelligence',\n",
       " 'Biomedical Sciences',\n",
       " 'Business Analytics',\n",
       " 'Communication and Information Studies',\n",
       " 'Computer Science',\n",
       " 'Econometrics and Data Science',\n",
       " 'Econometrics and Operations Research',\n",
       " 'Economics and Business Economics',\n",
       " 'History',\n",
       " 'International Business Administration',\n",
       " 'Literature and Society',\n",
       " 'Mathematics',\n",
       " 'Media, Art, Design and Architecture',\n",
       " 'Philosophy',\n",
       " 'Philosophy, Politics and Economics']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def open_listing_and_filter():\n",
    "    driver.get(LISTING_URL)\n",
    "    dismiss_cookies()\n",
    "    # wait for either dropdowns or results\n",
    "    WebDriverWait(driver, 30).until(\n",
    "        lambda d: d.find_elements(By.CSS_SELECTOR, \"div.sg-dropdown-title\")\n",
    "        or d.find_elements(By.CSS_SELECTOR, \".sg-search-result\")\n",
    "    )\n",
    "    # language\n",
    "    xp_lang = \"//div[contains(@class,'sg-dropdown-title')][.//span[contains(., 'Language')]]\"\n",
    "    click_el(WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, xp_lang))))\n",
    "    if not try_click(\"#LanguageEN0 + label\", timeout=2):\n",
    "        # fallback by label text\n",
    "        lab = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//label[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ','abcdefghijklmnopqrstuvwxyz'), 'english')]\"))\n",
    "        )\n",
    "        click_el(lab)\n",
    "    # faculty\n",
    "    xp_fac = \"//div[contains(@class,'sg-dropdown-title')][.//span[contains(., 'Faculty')]]\"\n",
    "    click_el(WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, xp_fac))))\n",
    "    for fac in [\"School of Business and Economics\", \"Faculty of Science\", \"Faculty of Humanities\"]:\n",
    "        lab = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, f\"//label[contains(., '{fac}')]\"))\n",
    "        )\n",
    "        click_el(lab)\n",
    "    time.sleep(1.0)\n",
    "\n",
    "def collect_programmes_two_pages():\n",
    "    items = []\n",
    "    seen = set()\n",
    "\n",
    "    def read_cards():\n",
    "        cards = driver.find_elements(By.CSS_SELECTOR, \".sg-search-result\")\n",
    "        out = []\n",
    "        for c in cards:\n",
    "            try:\n",
    "                title_el = c.find_element(By.CSS_SELECTOR, \".sg-mb-1\")\n",
    "                title = title_el.text.strip()\n",
    "                try:\n",
    "                    a = title_el.find_element(By.CSS_SELECTOR, \"a[href]\")\n",
    "                except Exception:\n",
    "                    a = c.find_element(By.CSS_SELECTOR, \"a[href]\")\n",
    "                href = a.get_attribute(\"href\")\n",
    "                if href and href not in seen:\n",
    "                    out.append({\"title\": title, \"url\": href})\n",
    "                    seen.add(href)\n",
    "            except Exception:\n",
    "                continue\n",
    "        return out\n",
    "\n",
    "    # page one\n",
    "    items.extend(read_cards())\n",
    "\n",
    "    # scroll to reveal paginator\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(0.8)\n",
    "\n",
    "    # click page 2 inside the exact paginator container selector you gave\n",
    "    try:\n",
    "        nav = driver.find_element(\n",
    "            By.CSS_SELECTOR,\n",
    "            \"body > div > div > div.grid-container > div > div > div > div.cell.small-12.large-8.xlarge-9 > div.sg-mt-2.sg-mt-m-3.sg-mt-l-8 > nav.sg-pagination.sg-mt-7.sg-mt-m-6.show-for-medium\"\n",
    "        )\n",
    "        # prefer visible link with text 2\n",
    "        candidates = []\n",
    "        candidates += nav.find_elements(By.XPATH, \".//a[normalize-space()='2']\")\n",
    "        candidates += nav.find_elements(By.XPATH, \".//button[normalize-space()='2']\")\n",
    "        candidates += nav.find_elements(By.CSS_SELECTOR, \"a[aria-label*='2'], button[aria-label*='2']\")\n",
    "        clicked = False\n",
    "        for el in candidates:\n",
    "            try:\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", el)\n",
    "                time.sleep(0.2)\n",
    "                el.click()\n",
    "                time.sleep(1.2)\n",
    "                clicked = True\n",
    "                break\n",
    "            except Exception:\n",
    "                continue\n",
    "        if not clicked:\n",
    "            # fallback next\n",
    "            for sel in [\"a[aria-label*='Next']\", \"button[aria-label*='Next']\",\n",
    "                        \"li.sg-pagination__next a\", \"button.sg-pagination__next\"]:\n",
    "                try:\n",
    "                    el = nav.find_element(By.CSS_SELECTOR, sel)\n",
    "                    driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", el)\n",
    "                    time.sleep(0.2)\n",
    "                    el.click()\n",
    "                    time.sleep(1.2)\n",
    "                    clicked = True\n",
    "                    break\n",
    "                except Exception:\n",
    "                    continue\n",
    "        if clicked:\n",
    "            items.extend(read_cards())\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return items\n",
    "\n",
    "open_listing_and_filter()\n",
    "programmes = collect_programmes_two_pages()\n",
    "print(\"Programmes found:\", len(programmes))\n",
    "[itm[\"title\"] for itm in programmes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35f70cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_tab_three(url):\n",
    "    base = url.split(\"#/tab=\")[0]\n",
    "    driver.get(f\"{base}#/tab=3\")\n",
    "    dismiss_cookies()\n",
    "    time.sleep(0.8)\n",
    "\n",
    "def list_tracks():\n",
    "    items = driver.find_elements(By.CSS_SELECTOR, \"#study-program .accordion > div\")\n",
    "    out = []\n",
    "    for tr in items:\n",
    "        label = safe_text(tr)\n",
    "        if is_minor_text(label):\n",
    "            continue\n",
    "        out.append(tr)\n",
    "    return out\n",
    "\n",
    "def expand_if_collapsed(container):\n",
    "    try:\n",
    "        content = container.find_element(By.CSS_SELECTOR, \".accordion-content\")\n",
    "    except Exception:\n",
    "        content = None\n",
    "\n",
    "    if content and content.is_displayed():\n",
    "        return\n",
    "\n",
    "    for sel in [\"button\", \".accordion-title\"]:\n",
    "        try:\n",
    "            btn = container.find_element(By.CSS_SELECTOR, sel)\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", btn)\n",
    "            time.sleep(0.2)\n",
    "            btn.click()\n",
    "            time.sleep(0.8)\n",
    "            break\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "\n",
    "def list_year_items(track_container):\n",
    "    try:\n",
    "        items = track_container.find_elements(By.CSS_SELECTOR, \".accordion .accordion-item, .accordion > div\")\n",
    "    except Exception:\n",
    "        return []\n",
    "    out = []\n",
    "    for it in items:\n",
    "        yl = safe_text(it)\n",
    "        if is_minor_text(yl):\n",
    "            continue\n",
    "        out.append(it)\n",
    "    return out\n",
    "\n",
    "def parse_visible_tables(scope_container):\n",
    "    # read all visible tables under the given container\n",
    "    out = []\n",
    "    contents = scope_container.find_elements(By.CSS_SELECTOR, \".accordion-content\")\n",
    "    vis = [c for c in contents if c.is_displayed()]\n",
    "    if not vis and contents:\n",
    "        vis = [contents[0]]\n",
    "\n",
    "    for cont in vis:\n",
    "        tables = cont.find_elements(By.CSS_SELECTOR, \"table tbody\")\n",
    "        for tb in tables:\n",
    "            if not tb.is_displayed():\n",
    "                continue\n",
    "            rows = tb.find_elements(By.CSS_SELECTOR, \"tr\")\n",
    "            for r in rows:\n",
    "                tds = r.find_elements(By.CSS_SELECTOR, \"td\")\n",
    "                if not tds:\n",
    "                    continue\n",
    "                # name\n",
    "                try:\n",
    "                    name = tds[0].find_element(By.CSS_SELECTOR, \"a\").text.strip()\n",
    "                except Exception:\n",
    "                    name = tds[0].text.strip()\n",
    "                # period\n",
    "                per = None\n",
    "                if len(tds) > 1:\n",
    "                    try:\n",
    "                        per_text = tds[1].find_element(By.CSS_SELECTOR, \"a\").text.strip()\n",
    "                    except Exception:\n",
    "                        per_text = tds[1].text.strip()\n",
    "                    m = re.search(r\"(\\d+)\", per_text)\n",
    "                    per = int(m.group(1)) if m else None\n",
    "                # ects\n",
    "                ects = None\n",
    "                if len(tds) > 2:\n",
    "                    try:\n",
    "                        ects_text = tds[2].find_element(By.CSS_SELECTOR, \"a\").text.strip()\n",
    "                    except Exception:\n",
    "                        ects_text = tds[2].text.strip()\n",
    "                    m = re.search(r\"(\\d+)\", ects_text)\n",
    "                    ects = int(m.group(1)) if m else None\n",
    "                # code\n",
    "                code = \"\"\n",
    "                if len(tds) > 3:\n",
    "                    try:\n",
    "                        code = tds[3].find_element(By.CSS_SELECTOR, \"a\").text.strip()\n",
    "                    except Exception:\n",
    "                        code = tds[3].text.strip()\n",
    "\n",
    "                if name:\n",
    "                    out.append({\"course_name\": name, \"period\": per, \"ects\": ects, \"code\": code})\n",
    "    return out\n",
    "\n",
    "def parse_studiegids_all_tracks_years(url):\n",
    "    open_tab_three(url)\n",
    "\n",
    "    out = []\n",
    "    visited = set()  # guards against double parsing if DOM reflows\n",
    "\n",
    "    tracks = list_tracks()\n",
    "    for tr in tracks:\n",
    "        tr_label = safe_text(tr)\n",
    "        if is_minor_text(tr_label):\n",
    "            continue\n",
    "\n",
    "        expand_if_collapsed(tr)\n",
    "\n",
    "        year_items = list_year_items(tr)\n",
    "        if not year_items:\n",
    "            # table directly under track\n",
    "            rows = parse_visible_tables(tr)\n",
    "            for row in rows:\n",
    "                key = (tr_label, row.get(\"course_name\"), row.get(\"code\"))\n",
    "                if key in visited:\n",
    "                    continue\n",
    "                visited.add(key)\n",
    "                row[\"track\"] = tr_label\n",
    "                row[\"year_label\"] = \"\"\n",
    "                out.append(row)\n",
    "            continue\n",
    "\n",
    "        for yi in year_items:\n",
    "            yl = safe_text(yi)\n",
    "            if is_minor_text(yl):\n",
    "                continue\n",
    "\n",
    "            expand_if_collapsed(yi)\n",
    "            rows = parse_visible_tables(yi)\n",
    "            for row in rows:\n",
    "                key = (tr_label, yl, row.get(\"course_name\"), row.get(\"code\"))\n",
    "                if key in visited:\n",
    "                    continue\n",
    "                visited.add(key)\n",
    "                row[\"track\"] = tr_label\n",
    "                row[\"year_label\"] = yl\n",
    "                out.append(row)\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7da73758",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = DATA_DIR / \"vu_cache\"\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def cache_path(url):\n",
    "    slug = re.sub(r\"[^a-zA-Z0-9]+\", \"_\", url.strip(\"/\"))[:180]\n",
    "    return CACHE_DIR / f\"{slug}.html\"\n",
    "\n",
    "def static_soup(url):\n",
    "    fp = cache_path(url)\n",
    "    if fp.exists():\n",
    "        html = fp.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        return BeautifulSoup(html, \"lxml\")\n",
    "    r = requests.get(url, headers=HEADERS, timeout=25)\n",
    "    r.raise_for_status()\n",
    "    fp.write_text(r.text, encoding=\"utf-8\")\n",
    "    return BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "def pick_vu_base(info_links):\n",
    "    for u in info_links:\n",
    "        if not u:\n",
    "            continue\n",
    "        if \"vu.nl\" in u and \"/education/bachelor/\" in u:\n",
    "            base = u.split(\"?\")[0].rstrip(\"/\")\n",
    "            for tail in [\"/curriculum\", \"/future\", \"/admissions\", \"/careers\"]:\n",
    "                if base.endswith(tail):\n",
    "                    base = base[: -len(tail)]\n",
    "            return base\n",
    "    return None\n",
    "\n",
    "def parse_vu_base(base_url):\n",
    "    soup = static_soup(base_url)\n",
    "    desc = \"\"\n",
    "    el = soup.select_one(\".xlarge-offset-0\")\n",
    "    if el:\n",
    "        desc = el.get_text(strip=True)\n",
    "    return {\"vunl_description\": desc}\n",
    "\n",
    "def parse_vu_curriculum(base_url):\n",
    "    url = base_url.rstrip(\"/\") + \"/curriculum\"\n",
    "    soup = static_soup(url)\n",
    "\n",
    "    desc = \"\"\n",
    "    el = soup.select_one(\".xlarge-offset-0\")\n",
    "    if el:\n",
    "        desc = el.get_text(strip=True)\n",
    "\n",
    "    year_blocks = [b.get_text(strip=True) for b in soup.select(\".xxlarge-6\")] or []\n",
    "\n",
    "    subjects = []\n",
    "    for card in soup.select(\".vuw-card-border-left\"):\n",
    "        text = card.get_text(\"\\n\", strip=True)\n",
    "        name = text.split(\"\\n\")[0].strip()\n",
    "        if is_minor_text(name):\n",
    "            continue\n",
    "        ects = None\n",
    "        period = None\n",
    "        year = None\n",
    "        m = re.search(r\"(\\d+)\\s*ECTS\", text, flags=re.I)\n",
    "        if m:\n",
    "            ects = int(m.group(1))\n",
    "        m = re.search(r\"Period\\s*([0-9]+)\", text, flags=re.I)\n",
    "        if m:\n",
    "            period = int(m.group(1))\n",
    "        m = re.search(r\"Year\\s*([0-9]+)\", text, flags=re.I)\n",
    "        if m:\n",
    "            year = int(m.group(1))\n",
    "        subjects.append({\"course_name\": name, \"ects\": ects, \"period\": period, \"year\": year})\n",
    "\n",
    "    return {\n",
    "        \"vunl_description_curriculum\": desc,\n",
    "        \"vunl_firstyear_description_blocks\": year_blocks,\n",
    "        \"vunl_subjects\": subjects\n",
    "    }\n",
    "\n",
    "def parse_vu_future(base_url):\n",
    "    url = base_url.rstrip(\"/\") + \"/future\"\n",
    "    soup = static_soup(url)\n",
    "    desc = \"\"\n",
    "    el = soup.select_one(\".xlarge-offset-0\")\n",
    "    if el:\n",
    "        desc = el.get_text(strip=True)\n",
    "    career = \"\"\n",
    "    el = soup.select_one(\".large-6 + .large-6 .vuw-p-6\")\n",
    "    if el:\n",
    "        career = el.get_text(\" \", strip=True)\n",
    "    return {\"vunl_future_description\": desc, \"vunl_future_career\": career}\n",
    "\n",
    "def parse_vu_admissions(base_url):\n",
    "    url = base_url.rstrip(\"/\") + \"/admissions\"\n",
    "    soup = static_soup(url)\n",
    "    rich_texts = [el.get_text(\" \", strip=True) for el in soup.select(\".vuw-rich-text\")]\n",
    "    dutch = \"\"\n",
    "    for t in rich_texts:\n",
    "        if \"Dutch\" in t or \"VWO\" in t or \"Dutch diploma\" in t:\n",
    "            dutch = t\n",
    "            break\n",
    "    return {\"vunl_admission_dutch_diploma\": dutch}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4087827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a smaller sample of the data\n",
    "programmes = programmes[:2]  # limit to first 2 for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5513d6",
   "metadata": {},
   "source": [
    "## 6. Main pipeline, all tracks and years, plus vu.nl enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e22d2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Programmes:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "rows_prog = []\n",
    "rows_subj = []\n",
    "\n",
    "for item in tqdm(programmes, desc=\"Programmes\"):\n",
    "    url = item[\"url\"]\n",
    "    title = item[\"title\"]\n",
    "\n",
    "    prog = {\"programme_title\": title, \"programme_url\": url}\n",
    "\n",
    "    # studiegids tab two for description and info links\n",
    "    base = url.split(\"#/tab=\")[0]\n",
    "    driver.get(f\"{base}#/tab=2\")\n",
    "    dismiss_cookies()\n",
    "    time.sleep(0.8)\n",
    "    try:\n",
    "        prog[\"sg_description\"] = driver.find_element(By.CSS_SELECTOR, \"#study-description\").text.strip()\n",
    "    except Exception:\n",
    "        prog[\"sg_description\"] = \"\"\n",
    "    info_links = []\n",
    "    try:\n",
    "        info_block = driver.find_element(By.CSS_SELECTOR, \".info\")\n",
    "        anchors = info_block.find_elements(By.CSS_SELECTOR, \"a[href]\")\n",
    "        info_links = [a.get_attribute(\"href\") for a in anchors]\n",
    "    except Exception:\n",
    "        pass\n",
    "    prog[\"info_links\"] = info_links\n",
    "\n",
    "    # subjects from all tracks and years\n",
    "    all_rows = parse_studiegids_all_tracks_years(url)\n",
    "    for r in all_rows:\n",
    "        rows_subj.append({\n",
    "            \"programme_title\": title,\n",
    "            \"programme_url\": url,\n",
    "            \"track\": r.get(\"track\", \"\"),\n",
    "            \"year_label\": r.get(\"year_label\", \"\"),\n",
    "            \"course_name\": r.get(\"course_name\"),\n",
    "            \"period\": r.get(\"period\"),\n",
    "            \"ects\": r.get(\"ects\"),\n",
    "            \"code\": r.get(\"code\")\n",
    "        })\n",
    "\n",
    "    # vu.nl enrichment\n",
    "    vu_base = pick_vu_base(info_links)\n",
    "    prog[\"vunl_base_url\"] = vu_base\n",
    "    if vu_base:\n",
    "        prog.update(parse_vu_base(vu_base))\n",
    "        cur = parse_vu_curriculum(vu_base)\n",
    "        prog.update({k: v for k, v in cur.items() if k != \"vunl_subjects\"})\n",
    "        fut = parse_vu_future(vu_base)\n",
    "        prog.update(fut)\n",
    "        prog.update(parse_vu_admissions(vu_base))\n",
    "\n",
    "    rows_prog.append(prog)\n",
    "\n",
    "df_prog = pd.DataFrame(rows_prog).drop_duplicates(subset=[\"programme_url\"]).reset_index(drop=True)\n",
    "df_subj = pd.DataFrame(rows_subj).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "print(\"Programmes shape\", df_prog.shape)\n",
    "print(\"Subjects shape\", df_subj.shape)\n",
    "\n",
    "df_prog.head(2), df_subj.head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
