{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4297891",
   "metadata": {},
   "source": [
    "# 4 Generating the Bank for the MicroTasks\n",
    "\n",
    "To generate the bank for the microtasks I will use an API for an LLM.\n",
    "The output will be two questions for each core course of each programme.\n",
    "\n",
    "Basically I will create the perfect prompt that will use the columns of the df_courses to generate the microstasks. \n",
    "\n",
    "We use two prompts: broad + disambiguaition (Kenneth Style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a37def8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "#!pip install --upgrade openai\n",
    "import os, re\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # optional progress bar, pip install tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6274530",
   "metadata": {},
   "source": [
    "## 1 Load the data and filter for max 2 courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209c821d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the csv file about the courses forwhih we have to gen the tasks\n",
    "silver = Path(\"../data_programmes_courses/silver\")\n",
    "\n",
    "df_courses_tasks = pd.read_csv(silver / \"df_courses_tasks_silver.csv\", encoding=\"utf-8-sig\")\n",
    "print(\"The shape of the courses tasks dataframe is:\", df_courses_tasks.shape)\n",
    "\n",
    "# keep only first two courses from each programme\n",
    "df_courses_tasks = df_courses_tasks.groupby(\"programme_title\").head(2).reset_index(drop=True)\n",
    "print(\"After keeping only first two courses from each programme the shape is:\", df_courses_tasks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1387546c",
   "metadata": {},
   "source": [
    "## 2. Set up OpenAI client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7c36b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "key_path = Path(\"../data_bank_microtasks\") / \"api_key.txt\"\n",
    "\n",
    "# Read the key and strip spaces and newlines\n",
    "api_key = key_path.read_text(encoding=\"utf8\").strip()\n",
    "\n",
    "# Create the client using this key\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "models = client.models.list()\n",
    "#for m in models.data:\n",
    "#    print(m.id)\n",
    "\n",
    "model_gpt = \"gpt-4.1-mini\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59136302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we list all programme names\n",
    "programmes = sorted(df_courses_tasks[\"programme_title\"].unique())\n",
    "print(\"Number of programmes:\", len(programmes))\n",
    "print(\"First few programmes:\", programmes[:5])\n",
    "\n",
    "def build_programme_context(df_prog: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    With this function we build a short text snippet that describes one programme.\n",
    "    We use the two core courses that we kept for that programme.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "\n",
    "    # we take the programme name from the first row\n",
    "    prog_name = df_prog[\"programme_title\"].iloc[0]\n",
    "    lines.append(f\"Programme: {prog_name}\")\n",
    "\n",
    "    # we loop over the two core courses\n",
    "    for idx, row in df_prog.iterrows():\n",
    "        course_title = row.get(\"course_name\", \"\")\n",
    "        course_obj = row.get(\"course_objective\", \"\")\n",
    "        course_cont = row.get(\"course_content\", \"\")\n",
    "\n",
    "        if isinstance(course_title, str) and course_title.strip():\n",
    "            lines.append(f\"Course: {course_title.strip()}\")\n",
    "\n",
    "        if isinstance(course_obj, str) and course_obj.strip():\n",
    "            # we keep the course objectives\n",
    "            lines.append(f\"Objectives: {course_obj.strip()}\")\n",
    "\n",
    "        if isinstance(course_cont, str) and course_cont.strip():\n",
    "            # we keep only a short part of the content to control prompt length\n",
    "            snippet = course_cont.strip()[:400]\n",
    "            lines.append(f\"Content snippet: {snippet}\")\n",
    "\n",
    "    # we join all lines in a single string\n",
    "    context = \"\\n\".join(lines)\n",
    "    return context\n",
    "\n",
    "# here we test the context builder for one programme\n",
    "test_prog = programmes[0]\n",
    "df_test = df_courses_tasks[df_courses_tasks[\"programme_title\"] == test_prog]\n",
    "print(\"Context for test programme:\")\n",
    "print(build_programme_context(df_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64df53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_aptitude_prompt(programme_name: str,\n",
    "                          programme_context: str,\n",
    "                          task_type: str) -> str:\n",
    "    \"\"\"\n",
    "    With this function we build the text that we send to the model\n",
    "    to create one aptitude microchallenge.\n",
    "    task_type can be \"classify\", \"fillblank\", or \"puzzle\".\n",
    "    \"\"\"\n",
    "    base = f\"\"\"\n",
    "You are creating an aptitude micro challenge for a high school student\n",
    "who is curious about the bachelor programme {programme_name}.\n",
    "\n",
    "You receive a short context that summarises real courses from this programme.\n",
    "You must anchor the challenge in that context.\n",
    "Do not invent random domains.\n",
    "Stay close to the topics and methods in the context.\n",
    "\n",
    "The task must test ability or reasoning, not personal preference.\n",
    "Use instructions such as \"Sort these\", \"Choose the correct\", \"Complete the text\".\n",
    "\n",
    "General requirements:\n",
    "- tiny_learn must be a list of exactly three short bullet points.\n",
    "  Each bullet explains one useful idea in simple language.\n",
    "- hint must be one short sentence that nudges the student without giving the answer away.\n",
    "- signalType must always be \"aptitude\".\n",
    "\"\"\"\n",
    "\n",
    "    if task_type == \"classify\":\n",
    "        specific = \"\"\"\n",
    "Task type: classify.\n",
    "\n",
    "You must return a JSON object with these fields:\n",
    "question_code: string, for example \"ancient-classify-001\"\n",
    "type: \"classify\"\n",
    "signalType: \"aptitude\"\n",
    "question: short instruction, for example \"Sort these into Greek or Roman origin\"\n",
    "tiny_learn: list of exactly three strings\n",
    "categories: list of category labels, for example [\"Greek\", \"Roman\"]\n",
    "items: list of objects with fields:\n",
    "    id: short id such as \"a\" or \"b\"\n",
    "    text: short description of the item\n",
    "    correctCategory: one of the category labels\n",
    "hint: short sentence\n",
    "\n",
    "The categories and items must make sense for this programme.\n",
    "\"\"\"\n",
    "    elif task_type == \"fillblank\":\n",
    "        specific = \"\"\"\n",
    "Task type: fillblank.\n",
    "\n",
    "You must return a JSON object with these fields:\n",
    "question_code: string, for example \"ancient-fillblank-001\"\n",
    "type: \"fillblank\"\n",
    "signalType: \"aptitude\"\n",
    "question: short instruction, for example \"Complete the text\"\n",
    "tiny_learn: list of exactly three strings\n",
    "textWithBlanks: short text that contains markers {{0}}, {{1}}, etc\n",
    "blanks: list of objects with fields:\n",
    "    id: integer index such as 0 or 1\n",
    "    correctWordId: id of the correct word from the words list\n",
    "words: list of objects with fields:\n",
    "    id: string, for example \"sumerian\"\n",
    "    text: the word as it should appear in the text\n",
    "hint: short sentence\n",
    "\n",
    "The text must describe something that fits the programme context.\n",
    "\"\"\"\n",
    "    elif task_type == \"puzzle\":\n",
    "        specific = \"\"\"\n",
    "Task type: puzzle.\n",
    "\n",
    "You must return a JSON object with these fields:\n",
    "question_code: string, for example \"ancient-puzzle-001\"\n",
    "type: \"puzzle\"\n",
    "signalType: \"aptitude\"\n",
    "question: short instruction, for example \"Which explanation fits best\"\n",
    "tiny_learn: list of exactly three strings\n",
    "puzzle: object that can include a stem or short description, for example:\n",
    "    { \"variant\": \"logic\", \"stem\": \"...\" }\n",
    "options: list of objects with fields:\n",
    "    id: short letter id, \"A\", \"B\", \"C\", \"D\"\n",
    "    value: the text of the option\n",
    "correctAnswer: id of the correct option\n",
    "hint: short sentence\n",
    "\n",
    "The puzzle must be solvable using the context and standard school knowledge.\n",
    "\"\"\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported task_type: {task_type}\")\n",
    "\n",
    "    context_block = f\"\"\"\n",
    "Programme context:\n",
    "{programme_context}\n",
    "\n",
    "Output format:\n",
    "Return a single valid JSON object and nothing else.\n",
    "\"\"\"\n",
    "\n",
    "    return base + specific + context_block\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343300ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def generate_aptitude_task_for_programme(programme_name: str,\n",
    "                                         programme_context: str,\n",
    "                                         task_type: str) -> dict:\n",
    "    \"\"\"\n",
    "    With this function we call the model one time and return one aptitude task\n",
    "    parsed as a Python dict.\n",
    "    \"\"\"\n",
    "    # here we build the full prompt, including programme context and task type\n",
    "    prompt = build_aptitude_prompt(programme_name, programme_context, task_type)\n",
    "\n",
    "    # here we call the model\n",
    "    response = client.responses.create(\n",
    "        model=model_gpt,\n",
    "        input=prompt,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    # here we take the text part of the first output and strip spaces\n",
    "    raw = response.output[0].content[0].text.strip()\n",
    "\n",
    "    # small debug print in case something goes wrong\n",
    "    # we can comment this out later\n",
    "    print(\"RAW MODEL OUTPUT START\")\n",
    "    print(raw[:500])\n",
    "    print(\"RAW MODEL OUTPUT END\")\n",
    "\n",
    "    # here we try to extract the JSON object from the raw text\n",
    "    # we look for the first curly brace and the last curly brace\n",
    "    start = raw.find(\"{\")\n",
    "    end = raw.rfind(\"}\") + 1\n",
    "\n",
    "    if start == -1 or end == 0:\n",
    "        raise ValueError(\"We did not find any JSON object in the model output\")\n",
    "\n",
    "    json_str = raw[start:end]\n",
    "\n",
    "    # here we parse the JSON substring into a Python dict\n",
    "    task = json.loads(json_str)\n",
    "\n",
    "    # here we enforce signalType and type fields from our side\n",
    "    task[\"signalType\"] = \"aptitude\"\n",
    "    task[\"type\"] = task_type\n",
    "\n",
    "    # here we normalise tiny_learn to three bullets\n",
    "    tiny = task.get(\"tiny_learn\", [])\n",
    "    if not isinstance(tiny, list):\n",
    "        tiny = [str(tiny)]\n",
    "    if len(tiny) > 3:\n",
    "        tiny = tiny[:3]\n",
    "    while len(tiny) < 3:\n",
    "        tiny.append(\"Extra note about the concept.\")\n",
    "    task[\"tiny_learn\"] = tiny\n",
    "\n",
    "    return task\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb6d0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prog = programmes[0]\n",
    "df_test = df_courses_tasks[df_courses_tasks[\"programme_title\"] == test_prog]\n",
    "ctx = build_programme_context(df_test)\n",
    "\n",
    "test_task = generate_aptitude_task_for_programme(\n",
    "    programme_name=test_prog,\n",
    "    programme_context=ctx,\n",
    "    task_type=\"classify\",\n",
    ")\n",
    "\n",
    "print(json.dumps(test_task, indent=2, ensure_ascii=False))\n",
    "print(\"tiny_learn:\", test_task.get(\"tiny_learn\"))\n",
    "print(\"Number of bullets:\", len(test_task.get(\"tiny_learn\", [])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fe7285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# here we decide which aptitude task types we want for each programme\n",
    "# we can change this list later if we add more types\n",
    "task_types = [\"classify\", \"fillblank\", \"puzzle\", ]\n",
    "\n",
    "aptitude_bank = {}\n",
    "\n",
    "# here we loop over all programmes and create aptitude tasks\n",
    "for prog in tqdm(programmes, desc=\"Generating aptitude microchallenges\"):\n",
    "    # we filter the two core courses for this programme\n",
    "    df_prog = df_courses_tasks[df_courses_tasks[\"programme_title\"] == prog]\n",
    "\n",
    "    # we build the short context snippet for the programme\n",
    "    ctx = build_programme_context(df_prog)\n",
    "\n",
    "    tasks_for_prog = []\n",
    "\n",
    "    # here we generate one task for each type in task_types\n",
    "    for task_type in task_types:\n",
    "        try:\n",
    "            task = generate_aptitude_task_for_programme(\n",
    "                programme_name=prog,\n",
    "                programme_context=ctx,\n",
    "                task_type=task_type,\n",
    "            )\n",
    "            tasks_for_prog.append(task)\n",
    "        except Exception as e:\n",
    "            # we print the problem and keep going with the next type or programme\n",
    "            print(f\"Problem for programme {prog} task type {task_type}: {e}\")\n",
    "\n",
    "    # we store the aptitude tasks for this programme\n",
    "    aptitude_bank[prog] = {\n",
    "        \"aptitude\": tasks_for_prog\n",
    "    }\n",
    "\n",
    "print(\"Example programme:\", programmes[0])\n",
    "print(json.dumps(aptitude_bank[programmes[0]][\"aptitude\"], indent=2, ensure_ascii=False))\n",
    "\n",
    "# ================================================================\n",
    "# here we merge aptitude with the existing personality bank\n",
    "# ================================================================\n",
    "\n",
    "data_dir = Path(\"../data_bank_microtasks\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# here we load the existing microtasks bank if it exists\n",
    "base_bank_path = data_dir / \"microtasks_bank.json\"\n",
    "\n",
    "if base_bank_path.exists():\n",
    "    with open(base_bank_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        microtasks_bank = json.load(f)\n",
    "    print(\"Loaded existing microtasks_bank.json\")\n",
    "else:\n",
    "    # if we do not have a previous bank we start from an empty dict\n",
    "    microtasks_bank = {}\n",
    "    print(\"No existing microtasks_bank.json found, we start from an empty bank\")\n",
    "\n",
    "# here we merge aptitude tasks into the main bank\n",
    "for prog, block in aptitude_bank.items():\n",
    "    # we make sure the programme entry exists in the main bank\n",
    "    if prog not in microtasks_bank:\n",
    "        microtasks_bank[prog] = {}\n",
    "    # we replace or create the aptitude list for this programme\n",
    "    microtasks_bank[prog][\"aptitude\"] = block[\"aptitude\"]\n",
    "\n",
    "# ================================================================\n",
    "# here we save the full bank\n",
    "# ================================================================\n",
    "\n",
    "full_bank_path = data_dir / \"microtasks_bank_full.json\"\n",
    "\n",
    "with open(full_bank_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(microtasks_bank, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved full microtasks bank to:\", full_bank_path)\n",
    "print(\"Number of programmes in the full bank:\", len(microtasks_bank))\n",
    "\n",
    "# here we quickly check that one programme has both personality and aptitude if personality existed\n",
    "sample_prog = programmes[0]\n",
    "print(\"Sample programme:\", sample_prog)\n",
    "print(\"Keys for this programme:\", microtasks_bank.get(sample_prog, {}).keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e33d572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# here we set the directory where we store all banks\n",
    "data_dir = Path(\"../data_bank_microtasks\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# here we define the paths of the two source jsons\n",
    "personality_path = data_dir / \"microtasks.json\"\n",
    "aptitude_path = data_dir / \"microchallenges_bank_aptitude.json\"\n",
    "\n",
    "# here we load the personality bank\n",
    "with open(personality_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    personality_bank = json.load(f)\n",
    "\n",
    "print(\"We loaded the personality bank with programmes:\",\n",
    "      len(personality_bank))\n",
    "\n",
    "# here we load the aptitude bank\n",
    "with open(aptitude_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    aptitude_bank = json.load(f)\n",
    "\n",
    "print(\"We loaded the aptitude bank with programmes:\",\n",
    "      len(aptitude_bank))\n",
    "\n",
    "# here we start from the personality bank as base\n",
    "full_bank = personality_bank.copy()\n",
    "\n",
    "# here we merge aptitude blocks into the full bank\n",
    "for prog, block in aptitude_bank.items():\n",
    "    # we make sure there is a dict for this programme\n",
    "    if prog not in full_bank:\n",
    "        full_bank[prog] = {}\n",
    "    # we take the aptitude list from the aptitude bank\n",
    "    full_bank[prog][\"aptitude\"] = block.get(\"aptitude\", [])\n",
    "\n",
    "# here we save the full merged bank\n",
    "full_bank_path = data_dir / \"microtasks_bank_full.json\"\n",
    "\n",
    "with open(full_bank_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(full_bank, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"We saved the full bank to:\", full_bank_path)\n",
    "\n",
    "# here we quickly inspect one programme to see the keys\n",
    "sample_prog = \"Ancient Studies\"\n",
    "if sample_prog in full_bank:\n",
    "    print(\"Sample programme:\", sample_prog)\n",
    "    print(\"Blocks for this programme:\", list(full_bank[sample_prog].keys()))\n",
    "else:\n",
    "    print(\"Warning, Ancient Studies is not in the merged bank\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
