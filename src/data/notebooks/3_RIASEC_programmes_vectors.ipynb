{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e49a307",
   "metadata": {},
   "source": [
    "# 3 Generate the RIASEC vectors on the programmes\n",
    "\n",
    "**Idea**: Lexicon score, concept and math\\\n",
    "\n",
    "RIASEC gives six interest areas Realistic, Investigative, Artistic, Social, Enterprising, Conventional. \n",
    "\n",
    "O*NETâ€™s Interest Profiler materials define these areas and show example activities and descriptors that map to each one. We will build a small dictionary of words and short phrases for each area, then score any program text by how much of that text overlaps with each dictionary. \n",
    "\n",
    "Method: **TF-IDF**\n",
    "\n",
    "**Output:**\n",
    "A six number vector per program. Each vector must be normalized: l2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324d69da",
   "metadata": {},
   "source": [
    "## 1. Imports, paths, and loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c417255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Files\n",
    "PATH_PROG = Path(r\"..\\data_programmes_courses\\silver\\df_programmes_silver.csv\")   # programmes table\n",
    "PATH_COUR = Path(r\"..\\data_programmes_courses\\silver\\df_courses_silver.csv\")      # courses table\n",
    "\n",
    "# Read data\n",
    "df_prog = pd.read_csv(PATH_PROG)\n",
    "df_cour = pd.read_csv(PATH_COUR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6352b19a",
   "metadata": {},
   "source": [
    "## 2. Text fields, cleaning, and programme texts by column\n",
    "Selects the exact programme columns you listed.\n",
    "\n",
    "Cleans each text field into a stable, lowercase, punctuation free form.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17e43b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Keep only the columns we care about\n",
    "PROG_FIELDS = [\n",
    "    \"sg_description\",\n",
    "    \"vunl_description\",\n",
    "    \"vunl_description_curriculum\",\n",
    "    \"vunl_future_description\",\n",
    "    \"vunl_future_career\",\n",
    "    \"year1_description\",\n",
    "    \"year2_description\",\n",
    "    \"year3_description\",\n",
    "]\n",
    "\n",
    "# Ensure all columns exist\n",
    "for c in PROG_FIELDS:\n",
    "    if c not in df_prog.columns:\n",
    "        df_prog[c] = \"\"\n",
    "\n",
    "COURSE_FIELDS = [\n",
    "    \"course_objective\",\n",
    "    \"course_content\",\n",
    "    \"method_of_assessment\",\n",
    "    \"recommended_background_knowledge\",\n",
    "]\n",
    "for c in COURSE_FIELDS:\n",
    "    if c not in df_cour.columns:\n",
    "        df_cour[c] = \"\"\n",
    "\n",
    "# Simple cleaner\n",
    "def clean_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"http[s]?://\\S+\", \" \", s)\n",
    "    s = re.sub(r\"[^a-z\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cf032a",
   "metadata": {},
   "source": [
    "\n",
    "## 3. RIASEC seed lexicon and a vectoriser\n",
    "\n",
    "- We make a big text per programme ans another per course. \n",
    "- Then we define a seed lexicon for the six letters.\n",
    "- Fits a TF IDF on the real corpus but with a restricted vocabulary. This enforces that only lexicon terms carry weight.\n",
    "- Stores the column indices for each letter so we can sum the TF IDF weights per letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f638a737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One combined programme text per row\n",
    "df_prog[\"programme_text_clean\"] = (\n",
    "    df_prog[PROG_FIELDS].fillna(\"\").agg(\" \".join, axis=1).apply(clean_text)\n",
    ")\n",
    "\n",
    "# One combined course text per row\n",
    "df_cour[\"course_text_clean\"] = (\n",
    "    df_cour[COURSE_FIELDS].fillna(\"\").agg(\" \".join, axis=1).apply(clean_text)\n",
    ")\n",
    "\n",
    "# Small seed lexicon. Expand later with O*NET style terms.\n",
    "LEX: Dict[str, List[str]] = {\n",
    "    \"R\": [\"lab\",\"field\",\"equipment\",\"tools\",\"build\",\"repair\",\"operate\",\"install\",\"measure\",\n",
    "          \"laboratory\",\"prototype\",\"machinery\",\"hardware\",\"electronics\",\"sample\",\"specimen\",\"safety\"\n",
    "          ,\"construction\",\"manual\",\"physical\",\"technician\",\"maintenance\",\"inspection\",\"diagnose\",\"weld\"],\n",
    "    \"I\": [\"analyze\",\"theory\",\"model\",\"proof\",\"derive\",\"experiment\",\"hypothesis\",\"data\",\n",
    "          \"research\",\"statistics\",\"algorithm\",\"simulate\",\"evidence\",\"inference\",\"mathematics\",\"physics\",\"logic\"\n",
    "          ,\"quantitative\",\"scientific\",\"compute\",\"computation\",\"evaluate\",\"study\",\"investigate\"],\n",
    "    \"A\": [\"design\",\"draw\",\"sketch\",\"compose\",\"write\",\"narrative\",\"visual\",\"media\",\"art\",\n",
    "          \"music\",\"film\",\"theatre\",\"creative\",\"story\",\"photography\",\"gallery\",\"curation\"\n",
    "          ,\"performance\",\"aesthetic\",\"illustrate\",\"exhibit\",\"craft\",\"fashion\",\"style\"],\n",
    "    \"S\": [\"help\",\"support\",\"advise\",\"coach\",\"teach\",\"tutor\",\"counsel\",\"community\",\"team\",\n",
    "          \"care\",\"wellbeing\",\"interview\",\"facilitate\",\"mentor\",\"outreach\",\"collaborate\",\"group\",\"clients\"\n",
    "          ,\"service\",\"social\",\"develop\",\"train\",\"educate\"],\n",
    "    \"E\": [\"business\",\"lead\",\"manage\",\"strategy\",\"sales\",\"marketing\",\"finance\",\"entrepreneurship\",\n",
    "          \"pitch\",\"negotiate\",\"market\",\"revenue\",\"growth\",\"product\",\"stakeholder\",\"budget\",\"plan\"\n",
    "          ,\"customer\",\"commercial\",\"operation\",\"organisational\",\"investor\",\"network\"],\n",
    "    \"C\": [\"organize\",\"detail\",\"procedure\",\"policy\",\"regulation\",\"compliance\",\"audit\",\"accounting\",\n",
    "          \"schedule\",\"record\",\"document\",\"database\",\"spreadsheet\",\"report\",\"inventory\",\"forms\",\"workflow\",\"quality\"\n",
    "          ,\"administration\",\"logistics\",\"systematic\",\"process\",\"standard\"],\n",
    "}\n",
    "\n",
    "\n",
    "LETTERS = [\"R\",\"I\",\"A\",\"S\",\"E\",\"C\"]\n",
    "\n",
    "# Restrict TF IDF to these words so the scores are easy to explain\n",
    "vocab = sorted({w for terms in LEX.values() for w in terms})\n",
    "corpus = pd.concat([\n",
    "    df_prog[\"programme_text_clean\"],\n",
    "    df_cour[\"course_text_clean\"]\n",
    "], ignore_index=True)\n",
    "\n",
    "tfidf = TfidfVectorizer(vocabulary=vocab, ngram_range=(1, 1), norm=\"l2\")\n",
    "tfidf.fit(corpus.tolist())\n",
    "\n",
    "# Map each letter to the feature indices\n",
    "feat_names = np.array(tfidf.get_feature_names_out())\n",
    "IDX = {L: np.where(np.isin(feat_names, LEX[L]))[0] for L in LETTERS}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0bc90f",
   "metadata": {},
   "source": [
    "## 4. Scoring functions, then programme and course vectors\n",
    "\n",
    "- With riasec_from_text we sum TF IDF weights for each letter and L2 normalizes the six numbers so the dot product can be used as cosine similarity. \n",
    "- Programme side uses one combined text. \n",
    "- Course side makes one vector per course, then averages with ECTS weights, then normalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78b1dfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def l2_normalize(vec: np.ndarray, eps=1e-8) -> np.ndarray:\n",
    "    z = np.sqrt((vec * vec).sum()) + eps\n",
    "    return vec / z if z > 0 else vec\n",
    "\n",
    "def riasec_from_text(text: str) -> np.ndarray:\n",
    "    \"\"\"Turn one cleaned text into a six number vector with L2 norm equal to 1.\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return np.zeros(6, dtype=float)\n",
    "    row = tfidf.transform([text])\n",
    "    sums = []\n",
    "    for L in LETTERS:\n",
    "        idx = IDX[L]\n",
    "        val = float(row[:, idx].sum()) if idx.size else 0.0\n",
    "        sums.append(val)\n",
    "    return l2_normalize(np.array(sums, dtype=float))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa3e4a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lmo820\\AppData\\Local\\Temp\\ipykernel_28880\\156056447.py:29: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(programme_course_vector)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Programme description vectors, one row per programme_title\n",
    "V_progdesc = (\n",
    "    df_prog\n",
    "      .groupby(\"programme_title\", as_index=False)\n",
    "      .agg(programme_text_clean=(\"programme_text_clean\",\"first\"))\n",
    ")\n",
    "V_progdesc[LETTERS] = V_progdesc[\"programme_text_clean\"].apply(riasec_from_text).apply(pd.Series)\n",
    "\n",
    "# Course vectors, credit weighted by ECTS, then one row per programme_title\n",
    "df_cour[\"ects\"] = pd.to_numeric(df_cour[\"ects\"], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "def programme_course_vector(group: pd.DataFrame) -> np.ndarray:\n",
    "    g = group[group[\"course_text_clean\"].str.len() > 0]\n",
    "    if g.empty:\n",
    "        return np.zeros(6, dtype=float)\n",
    "    weights = g[\"ects\"].to_numpy(dtype=float)\n",
    "    total = weights.sum()\n",
    "    if total <= 0:\n",
    "        weights = np.ones(len(g), dtype=float) / len(g)\n",
    "    else:\n",
    "        weights = weights / total\n",
    "    mats = np.vstack([riasec_from_text(t) for t in g[\"course_text_clean\"]])\n",
    "    vec = (weights.reshape(-1,1) * mats).sum(axis=0)\n",
    "    return l2_normalize(vec)\n",
    "\n",
    "V_courses = (\n",
    "    df_cour\n",
    "      .groupby(\"programme_title\")\n",
    "      .apply(programme_course_vector)\n",
    "      .reset_index(name=\"vec\")\n",
    ")\n",
    "V_courses[LETTERS] = V_courses[\"vec\"].apply(pd.Series)\n",
    "V_courses = V_courses.drop(columns=[\"vec\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f563a1be",
   "metadata": {},
   "source": [
    "## Cell 5. Final blend and save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff1edb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Join by programme_title\n",
    "P = V_progdesc.set_index(\"programme_title\")[LETTERS]\n",
    "C = V_courses.set_index(\"programme_title\")[LETTERS]\n",
    "titles = sorted(set(P.index) | set(C.index))\n",
    "P = P.reindex(titles).fillna(0.0)\n",
    "C = C.reindex(titles).fillna(0.0)\n",
    "\n",
    "# L2 blend for matching\n",
    "V_final_l2 = 0.5 * P.values + 0.5 * C.values\n",
    "V_final_l2 = np.vstack([l2_normalize(r) for r in V_final_l2]) # re-normalize\n",
    "DF_l2 = pd.DataFrame(V_final_l2, columns=LETTERS, index=titles).reset_index().rename(columns={\"index\":\"programme_title\"})\n",
    "\n",
    "# Quick checks\n",
    "assert np.allclose((DF_l2[LETTERS].to_numpy()**2).sum(axis=1), 1.0, atol=1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac556314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          programme_title  entropy_original   entropy  \\\n",
      "0                         Ancient Studies          1.643295  1.178040   \n",
      "1                             Archaeology          1.562317  1.170860   \n",
      "2                 Artificial Intelligence          1.619523  1.171021   \n",
      "3                     Biomedical Sciences          1.317086  1.178910   \n",
      "4                      Business Analytics          1.497401  1.171738   \n",
      "5   Communication and Information Studies          1.447639  1.172476   \n",
      "6                        Computer Science          1.502733  1.173959   \n",
      "7           Econometrics and Data Science          1.424194  1.176652   \n",
      "8    Econometrics and Operations Research          1.384730  1.178846   \n",
      "9        Economics and Business Economics          1.526821  1.171210   \n",
      "10                                History          1.563773  1.179533   \n",
      "11  International Business Administration          1.485759  1.181811   \n",
      "12                 Literature and Society          1.390083  1.174761   \n",
      "13                            Mathematics          1.124593  1.185171   \n",
      "14    Media, Art, Design and Architecture          1.009666  1.179183   \n",
      "15                             Philosophy          1.538899  1.174310   \n",
      "16     Philosophy, Politics and Economics          1.542461  1.180841   \n",
      "\n",
      "    temperature_used  \n",
      "0           0.170972  \n",
      "1           0.205117  \n",
      "2           0.224629  \n",
      "3           0.317310  \n",
      "4           0.234385  \n",
      "5           0.258774  \n",
      "6           0.312432  \n",
      "7           0.302676  \n",
      "8           0.327065  \n",
      "9           0.200239  \n",
      "10          0.136826  \n",
      "11          0.273408  \n",
      "12          0.273408  \n",
      "13          0.390479  \n",
      "14          0.390479  \n",
      "15          0.224629  \n",
      "16          0.202678  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# we list the columns that contain the vector entries\n",
    "# change these names if your DataFrame uses different ones\n",
    "VECTOR_COLS = [\"R\", \"I\", \"A\", \"S\", \"E\", \"C\"]\n",
    "\n",
    "# we set the target entropy (natural log base, same as in your colleague's code)\n",
    "TARGET_ENTROPY = 1.18\n",
    "TOL = 0.01\n",
    "\n",
    "\n",
    "def entropy_from_probs(p: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    We compute Shannon entropy using natural log.\n",
    "    We ignore zero entries to avoid log(0).\n",
    "    \"\"\"\n",
    "    p = np.array(p, dtype=float)\n",
    "    p = p[p > 0]\n",
    "    if p.size == 0:\n",
    "        return 0.0\n",
    "    return float(-np.sum(p * np.log(p)))\n",
    "\n",
    "\n",
    "def adjust_vector_to_target_entropy(vec: np.ndarray,\n",
    "                                   target_entropy: float = TARGET_ENTROPY,\n",
    "                                   tol: float = TOL):\n",
    "    \"\"\"\n",
    "    We take an existing vector and find a temperature T for softmax(v / T)\n",
    "    so that the entropy of the resulting probability vector is close to target_entropy.\n",
    "\n",
    "    We return:\n",
    "    - p: the probability vector that sums to one\n",
    "    - v_l2: the same vector rescaled to have L2 norm equal to one\n",
    "    - entropy: the final entropy we achieved\n",
    "    - T: the temperature we ended up using\n",
    "    \"\"\"\n",
    "    # we convert to a simple one dimensional numpy array\n",
    "    v = np.array(vec, dtype=float).reshape(-1)\n",
    "\n",
    "    # we set the search interval for the temperature\n",
    "    # small T gives very peaked distributions (low entropy)\n",
    "    # large T gives flatter distributions (high entropy)\n",
    "    T_low, T_high = 0.01, 10.0\n",
    "\n",
    "    p = None\n",
    "    entropy = None\n",
    "    T = None\n",
    "\n",
    "    # we run a fixed number of iterations of binary search\n",
    "    for _ in range(50):\n",
    "        T = (T_low + T_high) / 2.0\n",
    "\n",
    "        # we compute softmax(v / T)\n",
    "        x = v / T\n",
    "        x = x - x.max()  # we improve numerical stability\n",
    "        exp_x = np.exp(x)\n",
    "        p = exp_x / exp_x.sum()\n",
    "\n",
    "        # we compute entropy of this probability vector\n",
    "        entropy = entropy_from_probs(p)\n",
    "\n",
    "        # we check if we are close enough to the target\n",
    "        if abs(entropy - target_entropy) < tol:\n",
    "            break\n",
    "\n",
    "        # if entropy is greater than target, we want lower entropy, so we reduce T\n",
    "        if entropy > target_entropy:\n",
    "            T_high = T\n",
    "        # otherwise entropy is lower than target, we increase T\n",
    "        else:\n",
    "            T_low = T\n",
    "\n",
    "    # we finally create the L2 normalized version of p\n",
    "    norm = np.linalg.norm(p)\n",
    "    if norm == 0:\n",
    "        # we protect against division by zero, we fall back to uniform vector\n",
    "        v_l2 = np.ones_like(p) / np.sqrt(len(p))\n",
    "    else:\n",
    "        v_l2 = p / norm\n",
    "\n",
    "    return p, v_l2, entropy, T\n",
    "\n",
    "\n",
    "def adjust_row(row: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    We take one programme row, adjust its vector to have the target entropy,\n",
    "    and return a new row with updated vector and extra info.\n",
    "    \"\"\"\n",
    "    # we extract the original vector values as numpy array\n",
    "    original_vec = row[VECTOR_COLS].to_numpy(dtype=float)\n",
    "\n",
    "    # we compute the original entropy (for diagnostics)\n",
    "    original_probs = original_vec / original_vec.sum() if original_vec.sum() != 0 else np.ones_like(original_vec) / len(original_vec)\n",
    "    original_entropy = entropy_from_probs(original_probs)\n",
    "\n",
    "    # we adjust to target entropy\n",
    "    p, v_l2, new_entropy, T_used = adjust_vector_to_target_entropy(original_vec,\n",
    "                                                                   target_entropy=TARGET_ENTROPY,\n",
    "                                                                   tol=TOL)\n",
    "\n",
    "    # we create a copy of the row so we do not overwrite the original in place\n",
    "    new_row = row.copy()\n",
    "\n",
    "    # we replace the vector columns with the new L2 normalized vector\n",
    "    for col, value in zip(VECTOR_COLS, v_l2):\n",
    "        new_row[col] = value\n",
    "\n",
    "    # we store diagnostics for later inspection\n",
    "    new_row[\"entropy_original\"] = original_entropy\n",
    "    new_row[\"entropy\"] = new_entropy\n",
    "    new_row[\"temperature_used\"] = T_used\n",
    "\n",
    "    return new_row\n",
    "\n",
    "\n",
    "# we apply the adjustment to all programmes in the DataFrame\n",
    "df_adjusted = DF_l2.apply(adjust_row, axis=1)\n",
    "\n",
    "# we can quickly inspect the change in entropy\n",
    "print(df_adjusted[[\"programme_title\", \"entropy_original\", \"entropy\", \"temperature_used\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8a090c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove diagnostic columns before saving\n",
    "df_adjusted = df_adjusted.drop(columns=[\"entropy_original\", \"temperature_used\"]) \n",
    "# Save\n",
    "outdir = Path(r\"..\\data_RIASEC\")    \n",
    "outdir.mkdir(parents=True, exist_ok=True)\n",
    "df_adjusted.to_csv(outdir / \"df_RIASEC_programmes_vectors_adjusted.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46cbfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save\n",
    "outdir = Path(r\"..\\data_RIASEC\")\n",
    "outdir.mkdir(parents=True, exist_ok=True)\n",
    "DF_l2.to_csv(outdir / \"df_RIASEC_programmes_vectors.csv\", index=False)\n",
    "\n",
    "print(\"Saved:\", outdir / \"df_RIASEC_programmes_vectors.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
