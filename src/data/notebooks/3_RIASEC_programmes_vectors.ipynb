{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e49a307",
   "metadata": {},
   "source": [
    "# 3 Generate the RIASEC vectors on the programmes\n",
    "\n",
    "**Idea**: Lexicon score, concept and math\\\n",
    "RIASEC gives six interest areas Realistic, Investigative, Artistic, Social, Enterprising, Conventional. O*NET’s Interest Profiler materials define these areas and show example activities and descriptors that map to each one. We will build a small dictionary of words and short phrases for each area, then score any program text by how much of that text overlaps with each dictionary. That gives a six number vector per program.\n",
    "\n",
    "\n",
    "Let each course produce a continuous six number vector, then aggregate with weights.\n",
    "\n",
    "Per course vector v_course = scores from the lexicon over that course description.\n",
    "\n",
    "Weight per course w = ECTS credits times a year factor. For year weight use 1.0 for year 1, 0.6 for year 2, 0.4 for year 3.\n",
    "\n",
    "Programme course vector V_courses = weighted average of v_course over all required courses.\n",
    "\n",
    "Programme vector V_programme = 0.4 × scores from programme description plus 0.6 × V_courses.\n",
    "Tune these weights later using advisor feedback.\n",
    "\n",
    "This gives you a fair picture of what students will actually do, not only the marketing copy.\n",
    "\n",
    "Level 2. Improve quality with embeddings and metadata\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Core steps:**\n",
    "\n",
    "1. **Preprocess the text**\n",
    "Lowercase, strip markup, keep words, optional lemmatize.\n",
    "\n",
    "2. **Choose a lexicon for each letter**\n",
    "Example for Investigative might include analyze, theory, model, hypothesis, data, experiment. The O*NET Interest Profiler explains what belongs under each area and gives good guidance for building such seed sets.\n",
    "\n",
    "3. **Weight the words**\n",
    "    Two simple choices:\n",
    "    a. Raw counts per thousand tokens\n",
    "    b. TF IDF weights which downweight very common words and upweight informative ones\n",
    "\n",
    "4. **Aggregate to the six areas**\n",
    "For a program p and letter L, the raw score is the sum of weights for all tokens that appear in the L lexicon.\n",
    "\n",
    "5. **Normalize**\n",
    "Turn the six raw scores into proportions that sum to one. This gives a clean six number vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324d69da",
   "metadata": {},
   "source": [
    "## 1. Imports, paths, and JSON loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c417255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1. Imports, paths, and a robust JSON reader\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Files\n",
    "PATH_PROG = Path(r\".\\data\\df_programmes_silver.json\")   # programmes table\n",
    "PATH_COUR = Path(r\".\\data\\df_courses_silver.json\")      # courses table\n",
    "\n",
    "def read_json_table(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read either a JSON list or a JSON lines file into a DataFrame.\n",
    "    Keeps this flexible so the scraper format can evolve.\n",
    "    \"\"\"\n",
    "    raw = path.read_text(encoding=\"utf-8\").strip()\n",
    "    if raw.startswith(\"[\"):\n",
    "        return pd.DataFrame(json.loads(raw))\n",
    "    rows = [json.loads(x) for x in raw.splitlines() if x]\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df_prog = read_json_table(PATH_PROG)\n",
    "df_courses = read_json_table(PATH_COUR)\n",
    "\n",
    "# Minimal sanity checks\n",
    "assert \"programme_title\" in df_prog.columns, \"programme_title missing in df_programmes.json\"\n",
    "assert \"programme_title\" in df_courses.columns, \"programme_title missing in df_courses.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6352b19a",
   "metadata": {},
   "source": [
    "## 2. Text fields, cleaning, and programme texts by column\n",
    "Selects the exact programme columns you listed.\n",
    "\n",
    "Cleans each text field into a stable, lowercase, punctuation free form.\n",
    "\n",
    "Produces a long table with one row per programme per field.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f638a737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "programme_title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "field",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "2815be5f-f82c-45d4-8e45-51b76898d7c4",
       "rows": [
        [
         "0",
         "Ancient Studies",
         "sg_description",
         "the ancient world and its heritage are the focal points within the programme of ancient studies you learn to critically reflect upon processes of cultural interaction in antiquity and beyond with specific attention to the mediterranean world and to the various ways in which its rich and innovative cultures have contributed to historical and cultural developments in later times to study such processes you are trained to use an interdisciplinary approach that combines various perspectives theories and methodologies for this purpose you acquire a thorough knowledge of the ancient world s cultural historical material textual and philosophical aspects in the first year you study the sources and cultural history of antiquity and their continued impact from both a disciplinary and an interdisciplinary perspective in the second semester there is time to get acquainted with the various options for specialisation in the second and third year of the ancient studies programme you specialise in one of the adjacent disciplines offered in antiquity ancient history archaeology akkadian and ancient near eastern studies greek latin thus you obtain sufficient in depth knowledge in addition to your unique interdisciplinary perspective furthermore you choose a minor or take electives ec finally you write a thesis in which you study a subject related to antiquity from an interdisciplinary perspective acasa the ba programme ancient studies is offered by the amsterdam centre for ancient studies and archaeology acasa the academic centre that brings together the expertise of uva and vu in the fields of ancient studies classics and archaeology all courses are offered at the uva campus in the centre of amsterdam"
        ],
        [
         "1",
         "Archaeology",
         "sg_description",
         "archaeology as a discipline focuses upon the study of human societies cultures and landscapes throughout the ages with specific attention to their material remains it combines the application of theoretical models with a training in practical skills it is a multi disciplinary field of study that includes aspects of academic fields such as heritage landscape art and history within the archaeology programme you acquire a thorough knowledge of the past as well as an overview of archaeological methods and techniques in the first year you learn about the discipline itself you study societies of the european past follow courses in cultural history and learn how to deal with source material in the summer you round off the year with field work in the second and third year of your programme you obtain a more in depth knowledge of important themes within archaeology you further your practical skills and expertise and you learn how to use digital methods in your research for instance the mapping of data in gis you also participate in excursions and fieldwork campaigns abroad furthermore you choose a minor or take electives ec finally you write a thesis about a subject of your preference within the field of archaeology acasa the ba programme archaeology is offered by the amsterdam centre for ancient studies and archaeology acasa the academic centre that brings together the expertise of uva and vu in the fields of ancient studies classics and archaeology all courses are offered at the uva campus in the centre of amsterdam"
        ],
        [
         "2",
         "Artificial Intelligence",
         "sg_description",
         "artificial intelligence ai is the science of building and analysing intelligence demonstrated by machines how could alphago beat the best human go player is it possible to build a social robots that seamlessly talk to an elderly with dementia and help her to feel less lonely do you want to understand the algorithms google applies for its search or netflix to recommend us the latest movies in the first year students learn basics in computer science and mathematics such as programming and logic they also get acquainted with some of the main concepts in artificial intelligence such as modelling knowledge representation and machine learning as well as interaction between humans and machines in the second year students are able to choose between the intelligent systems specialisation which focuses on methods to build intelligent agents and the socially aware computing specialisation with an emphasis on ai in a social context with applications in health and law in the third year students follow one of the many minors offered by the university or the faculty including an educational minor which gives them a second degree teaching qualification for secondary school and our situated ai minor but many students take the opportunity to study abroad for one semester the final semester is mostly dedicated to the final bachelor project"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>programme_title</th>\n",
       "      <th>field</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ancient Studies</td>\n",
       "      <td>sg_description</td>\n",
       "      <td>the ancient world and its heritage are the foc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Archaeology</td>\n",
       "      <td>sg_description</td>\n",
       "      <td>archaeology as a discipline focuses upon the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>sg_description</td>\n",
       "      <td>artificial intelligence ai is the science of b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           programme_title           field  \\\n",
       "0          Ancient Studies  sg_description   \n",
       "1              Archaeology  sg_description   \n",
       "2  Artificial Intelligence  sg_description   \n",
       "\n",
       "                                                text  \n",
       "0  the ancient world and its heritage are the foc...  \n",
       "1  archaeology as a discipline focuses upon the s...  \n",
       "2  artificial intelligence ai is the science of b...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 2. Select programme columns, clean text, and build per-field texts\n",
    "\n",
    "# Programme columns we will score\n",
    "PROG_FIELDS = [\n",
    "    \"sg_description\",\n",
    "    \"vunl_description\",\n",
    "    \"vunl_description_curriculum\",\n",
    "    \"vunl_future_description\",\n",
    "    \"vunl_future_career\",\n",
    "    \"year1_description\",\n",
    "    \"year2_description\",\n",
    "    \"year3_description\",\n",
    "]\n",
    "\n",
    "# Create empty columns if any are missing\n",
    "for col in PROG_FIELDS:\n",
    "    if col not in df_prog.columns:\n",
    "        df_prog[col] = None\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Lowercase, remove urls, keep letters and spaces, collapse whitespace.\n",
    "    This keeps tokenisation stable for TF IDF with a small vocabulary.\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"http[s]?://\\S+\", \" \", s)\n",
    "    s = re.sub(r\"[^a-z\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# Cleaned versions of each programme level field\n",
    "for col in PROG_FIELDS:\n",
    "    df_prog[f\"clean__{col}\"] = df_prog[col].fillna(\"\").apply(clean_text)\n",
    "\n",
    "# For later convenience keep a long shaped view of programme texts by field\n",
    "prog_long = (\n",
    "    df_prog[[\"programme_title\"] + [f\"clean__{c}\" for c in PROG_FIELDS]]\n",
    "      .rename(columns={f\"clean__{c}\": c for c in PROG_FIELDS})\n",
    "      .melt(id_vars=\"programme_title\", var_name=\"field\", value_name=\"text\")\n",
    ")\n",
    "\n",
    "# Drop empty rows to avoid zero only documents in the corpus\n",
    "prog_long = prog_long[prog_long[\"text\"].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "prog_long.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a76a61c",
   "metadata": {},
   "source": [
    "\n",
    "## 3. RIASEC seed lexicon and a vectoriser that knows only these terms\n",
    "\n",
    "Defines a seed lexicon for the six letters.\n",
    "\n",
    "Adds a tiny set of two word phrases that capture strong signals.\n",
    "\n",
    "Fits a TF IDF on the real corpus but with a restricted vocabulary. This enforces that only lexicon terms carry weight.\n",
    "\n",
    "Stores the column indices for each letter so we can sum the TF IDF weights per letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4bc1398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148, {'R': 25, 'I': 24, 'A': 24, 'S': 23, 'E': 23, 'C': 23})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 3. RIASEC lexicon and TF IDF vectoriser\n",
    "\n",
    "# Small seed lexicon. Expand later with O*NET style terms and Dutch mates.\n",
    "LEXICON: Dict[str, List[str]] = {\n",
    "    \"R\": [\"lab\",\"field\",\"equipment\",\"tools\",\"build\",\"repair\",\"operate\",\"install\",\"measure\",\n",
    "          \"laboratory\",\"prototype\",\"machinery\",\"hardware\",\"electronics\",\"sample\",\"specimen\",\"safety\"\n",
    "          ,\"construction\",\"manual\",\"physical\",\"technician\",\"maintenance\",\"inspection\",\"diagnose\",\"weld\"],\n",
    "    \"I\": [\"analyze\",\"theory\",\"model\",\"proof\",\"derive\",\"experiment\",\"hypothesis\",\"data\",\n",
    "          \"research\",\"statistics\",\"algorithm\",\"simulate\",\"evidence\",\"inference\",\"mathematics\",\"physics\",\"logic\"\n",
    "          ,\"quantitative\",\"scientific\",\"compute\",\"computation\",\"evaluate\",\"study\",\"investigate\"],\n",
    "    \"A\": [\"design\",\"draw\",\"sketch\",\"compose\",\"write\",\"narrative\",\"visual\",\"media\",\"art\",\n",
    "          \"music\",\"film\",\"theatre\",\"creative\",\"story\",\"photography\",\"gallery\",\"curation\"\n",
    "          ,\"performance\",\"aesthetic\",\"illustrate\",\"exhibit\",\"craft\",\"fashion\",\"style\"],\n",
    "    \"S\": [\"help\",\"support\",\"advise\",\"coach\",\"teach\",\"tutor\",\"counsel\",\"community\",\"team\",\n",
    "          \"care\",\"wellbeing\",\"interview\",\"facilitate\",\"mentor\",\"outreach\",\"collaborate\",\"group\",\"clients\"\n",
    "          ,\"service\",\"social\",\"develop\",\"train\",\"educate\"],\n",
    "    \"E\": [\"business\",\"lead\",\"manage\",\"strategy\",\"sales\",\"marketing\",\"finance\",\"entrepreneurship\",\n",
    "          \"pitch\",\"negotiate\",\"market\",\"revenue\",\"growth\",\"product\",\"stakeholder\",\"budget\",\"plan\"\n",
    "          ,\"customer\",\"commercial\",\"operation\",\"organisational\",\"investor\",\"network\"],\n",
    "    \"C\": [\"organize\",\"detail\",\"procedure\",\"policy\",\"regulation\",\"compliance\",\"audit\",\"accounting\",\n",
    "          \"schedule\",\"record\",\"document\",\"database\",\"spreadsheet\",\"report\",\"inventory\",\"forms\",\"workflow\",\"quality\"\n",
    "          ,\"administration\",\"logistics\",\"systematic\",\"process\",\"standard\"],\n",
    "}\n",
    "\n",
    "# Optional short list of two word phrases that are very diagnostic\n",
    "PHRASES: List[str] = [\n",
    "    \"field work\",\n",
    "    \"case study\",\n",
    "    \"data analysis\",\n",
    "    \"random assignment\",\n",
    "    \"time series\",\n",
    "    \"quality control\",\n",
    "]\n",
    "\n",
    "def build_vectorizer(corpus_texts: Iterable[str], lexicon: Dict[str, List[str]], phrases: List[str]) -> tuple[TfidfVectorizer, np.ndarray, Dict[str, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Fit a TF IDF on the actual corpus, but restrict the vocabulary to the lexicon terms and chosen phrases.\n",
    "    Returns the fitted vectoriser, the feature name array, and indices for each letter.\n",
    "    \"\"\"\n",
    "    vocab = sorted(set(sum(lexicon.values(), [])) | set(phrases))\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        vocabulary=vocab,\n",
    "        ngram_range=(1, 2),   # allow the phrases\n",
    "        norm=\"l2\",\n",
    "        min_df=1\n",
    "    )\n",
    "    vectorizer.fit(list(corpus_texts))\n",
    "    feats = np.array(vectorizer.get_feature_names_out())\n",
    "    letter_to_idx = {L: np.where(np.isin(feats, lexicon[L]))[0] for L in [\"R\",\"I\",\"A\",\"S\",\"E\",\"C\"]}\n",
    "    return vectorizer, feats, letter_to_idx\n",
    "\n",
    "# Build a corpus using all programme texts and later we will extend with course texts before scoring courses\n",
    "corpus_prog = prog_long[\"text\"].tolist()\n",
    "tfidf_prog, feat_names_prog, idx_prog = build_vectorizer(corpus_prog, LEXICON, PHRASES)\n",
    "\n",
    "len(feat_names_prog), {k: len(v) for k, v in idx_prog.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82abc423",
   "metadata": {},
   "source": [
    "## 4. Scoring and aggregation functions for programme and course vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a384e094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4. Scoring helpers and the two aggregations: V_progdesc and V_courses\n",
    "\n",
    "LETTERS = [\"R\",\"I\",\"A\",\"S\",\"E\",\"C\"]\n",
    "\n",
    "def riasec_props_from_text(text: str, vectorizer: TfidfVectorizer, letter_to_idx: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute p(T) in R^6 for a single text.\n",
    "    1. Transform text to TF IDF row\n",
    "    2. Sum weights over the feature indices for each letter\n",
    "    3. Normalise to unit sum with a tiny epsilon\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return np.zeros(6, dtype=float)\n",
    "    row = vectorizer.transform([text])\n",
    "    sums = []\n",
    "    for L in LETTERS:\n",
    "        idx = letter_to_idx[L]\n",
    "        val = float(row[:, idx].sum()) if idx.size else 0.0\n",
    "        sums.append(val)\n",
    "    v = np.array(sums, dtype=float)\n",
    "    denom = v.sum() + 1e-8\n",
    "    return v / denom if denom > 0 else np.zeros(6, dtype=float)\n",
    "\n",
    "def aggregate_programme_description(df_prog: pd.DataFrame, fields: List[str], vectorizer: TfidfVectorizer, letter_to_idx: Dict[str, np.ndarray], alpha: Dict[str, float] | None = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute V_progdesc for each programme_title as a weighted blend of p(T_k) over selected programme fields.\n",
    "    If alpha is None, use equal weights over nonempty fields per programme.\n",
    "    Returns a DataFrame with one row per programme and six columns R,I,A,S,E,C.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for title, group in df_prog.groupby(\"programme_title\", dropna=False):\n",
    "        pieces = []\n",
    "        weights = []\n",
    "        for col in fields:\n",
    "            txt = group[f\"clean__{col}\"].iloc[0] if f\"clean__{col}\" in group.columns else \"\"\n",
    "            if not txt:\n",
    "                continue\n",
    "            pieces.append(riasec_props_from_text(txt, vectorizer, letter_to_idx))\n",
    "            if alpha is None:\n",
    "                weights.append(1.0)\n",
    "            else:\n",
    "                weights.append(alpha.get(col, 0.0))\n",
    "        if not pieces:\n",
    "            vec = np.zeros(6, dtype=float)\n",
    "        else:\n",
    "            w = np.array(weights, dtype=float)\n",
    "            if alpha is None:\n",
    "                w = w / w.sum()\n",
    "            vec = np.average(np.vstack(pieces), axis=0, weights=w)\n",
    "            # renormalise for safety\n",
    "            s = vec.sum() + 1e-8\n",
    "            vec = vec / s if s > 0 else vec\n",
    "        rows.append({\"programme_title\": title, **{LETTERS[i]: vec[i] for i in range(6)}})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def make_course_text(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Concatenate the course text fields we want to score.\n",
    "    These are the highest value fields with the least boilerplate.\n",
    "    \"\"\"\n",
    "    parts = [\n",
    "        row.get(\"course_objective\", \"\"),\n",
    "        row.get(\"course_content\", \"\"),\n",
    "        row.get(\"method_of_assessment\", \"\"),\n",
    "        row.get(\"recommended_background_knowledge\", \"\")\n",
    "    ]\n",
    "    return clean_text(\" \".join([p for p in parts if isinstance(p, str)]))\n",
    "\n",
    "def aggregate_courses_by_programme(df_cour: pd.DataFrame, vectorizer: TfidfVectorizer, letter_to_idx: Dict[str, np.ndarray]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute V_courses for each programme as a credits weighted mean of course vectors p(T_c).\n",
    "    w_c equals ects_c divided by total ects in the programme.\n",
    "    Returns one row per programme with columns R I A S E C.\n",
    "    \"\"\"\n",
    "    # Prepare cleaned course text and numeric ects\n",
    "    df = df_cour.copy()\n",
    "    for col in [\"ects\"]:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0.0)\n",
    "    df[\"course_text_clean\"] = df.apply(make_course_text, axis=1)\n",
    "\n",
    "    rows = []\n",
    "    for title, group in df.groupby(\"programme_title\", dropna=False):\n",
    "        group = group[group[\"course_text_clean\"].str.len() > 0]\n",
    "        if group.empty:\n",
    "            vec = np.zeros(6, dtype=float)\n",
    "        else:\n",
    "            ects = group[\"ects\"].to_numpy(dtype=float)\n",
    "            # avoid zero total with a tiny epsilon\n",
    "            total = ects.sum()\n",
    "            if total <= 0:\n",
    "                w = np.ones(len(group), dtype=float) / len(group)\n",
    "            else:\n",
    "                w = ects / total\n",
    "            mats = []\n",
    "            for txt in group[\"course_text_clean\"]:\n",
    "                mats.append(riasec_props_from_text(txt, vectorizer, letter_to_idx))\n",
    "            M = np.vstack(mats)\n",
    "            vec = np.average(M, axis=0, weights=w)\n",
    "            s = vec.sum() + 1e-8\n",
    "            vec = vec / s if s > 0 else vec\n",
    "        rows.append({\"programme_title\": title, **{LETTERS[i]: vec[i] for i in range(6)}})\n",
    "    return pd.DataFrame(rows)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
