{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b718d21a",
   "metadata": {},
   "source": [
    "# Scraping of Programmes and Courses Data\n",
    "\n",
    "Notebook pipeline that\n",
    "\n",
    "1) Loads both pages in studiegids and collects all programme URLs\n",
    "2) Clicks through every programme, then every track, then every year, and reads the visible tables\n",
    "3) Enriches each programme with text from vu.nl base, curriculum, future, and admissions\n",
    "4) Saves tidy CSV and JSON for programmes and subjects "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2481f670",
   "metadata": {},
   "source": [
    "## 1. Imports and setup\n",
    "Sets folders, constants, and starts Selenium Chrome.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a25700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libs\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import pathlib\n",
    "from urllib.parse import unquote\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "# selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8865646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# folders\n",
    "BASE_DIR = pathlib.Path().resolve()\n",
    "DATA_DIR = BASE_DIR / \"data_raw\"\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# start pages\n",
    "LISTING_URL = \"https://studiegids.vu.nl/en/bachelor/2025-2026#/\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# selenium driver\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless=new\")  # uncomment for headless runs\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "wait = WebDriverWait(driver, 25) #in seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd81a90d",
   "metadata": {},
   "source": [
    "## 2. Selenium helpers\n",
    "Utility functions for waiting, clicking, and dismissing cookies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd3ee3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q(css, timeout=25):\n",
    "    return WebDriverWait(driver, timeout).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, css))\n",
    "    )\n",
    "\n",
    "def q_all(css, timeout=25):\n",
    "    WebDriverWait(driver, timeout).until(\n",
    "        EC.presence_of_all_elements_located((By.CSS_SELECTOR, css))\n",
    "    )\n",
    "    return driver.find_elements(By.CSS_SELECTOR, css)\n",
    "\n",
    "def click_el(el):\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", el)\n",
    "    time.sleep(0.2)\n",
    "    el.click()\n",
    "\n",
    "def try_click(css, timeout=3):\n",
    "    try:\n",
    "        el = WebDriverWait(driver, timeout).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, css))\n",
    "        )\n",
    "        click_el(el)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def dismiss_cookies():\n",
    "    def click_buttons():\n",
    "        xpaths = [\n",
    "            \"//button[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ','abcdefghijklmnopqrstuvwxyz'),'accept')]\",\n",
    "            \"//button[contains(., 'Akkoord')]\",\n",
    "            \"//button[contains(., 'Alles accepteren')]\",\n",
    "            \"//button[contains(., 'Accept all')]\",\n",
    "            \"//button[contains(., 'Accept')]\",\n",
    "        ]\n",
    "        for xp in xpaths:\n",
    "            try:\n",
    "                btn = WebDriverWait(driver, 2).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, xp))\n",
    "                )\n",
    "                click_el(btn)\n",
    "                return True\n",
    "            except Exception:\n",
    "                pass\n",
    "        return False\n",
    "\n",
    "    if click_buttons():\n",
    "        return\n",
    "    frames = driver.find_elements(By.CSS_SELECTOR, \"iframe\")\n",
    "    for fr in frames:\n",
    "        try:\n",
    "            driver.switch_to.frame(fr)\n",
    "            if click_buttons():\n",
    "                driver.switch_to.default_content()\n",
    "                return\n",
    "        except Exception:\n",
    "            driver.switch_to.default_content()\n",
    "        finally:\n",
    "            driver.switch_to.default_content()\n",
    "\n",
    "\n",
    "# helper to detect Minor sections\n",
    "def is_minor_text(txt):\n",
    "    return bool(re.search(r\"\\bminor\\b\", (txt or \"\"), flags=re.I))\n",
    "\n",
    "def safe_text(el, sel=\".accordion-title\"):\n",
    "    try:\n",
    "        t = el.find_element(By.CSS_SELECTOR, sel).text.strip()\n",
    "        if t:\n",
    "            return t\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return el.text.strip()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7815bf0e",
   "metadata": {},
   "source": [
    "## 3. Listing filters and pagination\n",
    "\n",
    "Opens the bachelor listing, applies English and faculty filters. Then it scrolls, then clicks page two inside the exact paginator.\n",
    "Collects programme titles and URLs across both pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c1ce8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the faculties to include\n",
    "faculties_to_include = [\"School of Business and Economics\", \"Faculty of Science\", \"Faculty of Humanities\"]\n",
    "\n",
    "\n",
    "def open_listing_and_filter():\n",
    "    driver.get(LISTING_URL)\n",
    "    dismiss_cookies()\n",
    "    # wait for either dropdowns or results\n",
    "    WebDriverWait(driver, 30).until(\n",
    "        lambda d: d.find_elements(By.CSS_SELECTOR, \"div.sg-dropdown-title\")\n",
    "        or d.find_elements(By.CSS_SELECTOR, \".sg-search-result\")\n",
    "    )\n",
    "    # language of the courses\n",
    "    xp_lang = \"//div[contains(@class,'sg-dropdown-title')][.//span[contains(., 'Language')]]\"\n",
    "    click_el(WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, xp_lang))))\n",
    "    if not try_click(\"#LanguageEN0 + label\", timeout=2):\n",
    "        # fallback by label text\n",
    "        lab = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//label[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ','abcdefghijklmnopqrstuvwxyz'), 'english')]\"))\n",
    "        )\n",
    "        click_el(lab)\n",
    "    # faculty\n",
    "    xp_fac = \"//div[contains(@class,'sg-dropdown-title')][.//span[contains(., 'Faculty')]]\"\n",
    "    click_el(WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, xp_fac))))\n",
    "    for fac in faculties_to_include:\n",
    "        lab = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, f\"//label[contains(., '{fac}')]\"))\n",
    "        )\n",
    "        click_el(lab)\n",
    "    time.sleep(1.0)\n",
    "\n",
    "def collect_programmes_two_pages():\n",
    "    items = []\n",
    "    seen = set()\n",
    "\n",
    "    def read_cards():\n",
    "        cards = driver.find_elements(By.CSS_SELECTOR, \".sg-search-result\")\n",
    "        out = []\n",
    "        for c in cards:\n",
    "            try:\n",
    "                title_el = c.find_element(By.CSS_SELECTOR, \".sg-mb-1\")\n",
    "                title = title_el.text.strip()\n",
    "                try:\n",
    "                    a = title_el.find_element(By.CSS_SELECTOR, \"a[href]\")\n",
    "                except Exception:\n",
    "                    a = c.find_element(By.CSS_SELECTOR, \"a[href]\")\n",
    "                href = a.get_attribute(\"href\")\n",
    "                if href and href not in seen:\n",
    "                    out.append({\"title\": title, \"url\": href})\n",
    "                    seen.add(href)\n",
    "            except Exception:\n",
    "                continue\n",
    "        return out\n",
    "\n",
    "    # page one\n",
    "    items.extend(read_cards())\n",
    "\n",
    "    # scroll to reveal paginator\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(1.0)\n",
    "\n",
    "    # click page 2 inside the exact paginator container selector you gave\n",
    "    try:\n",
    "        nav = driver.find_element(\n",
    "            By.CSS_SELECTOR,\n",
    "            \"body > div > div > div.grid-container > div > div > div > div.cell.small-12.large-8.xlarge-9 > div.sg-mt-2.sg-mt-m-3.sg-mt-l-8 > nav.sg-pagination.sg-mt-7.sg-mt-m-6.show-for-medium\"\n",
    "        )\n",
    "        # prefer visible link with text 2\n",
    "        candidates = []\n",
    "        candidates += nav.find_elements(By.XPATH, \".//a[normalize-space()='2']\")\n",
    "        candidates += nav.find_elements(By.XPATH, \".//button[normalize-space()='2']\")\n",
    "        candidates += nav.find_elements(By.CSS_SELECTOR, \"a[aria-label*='2'], button[aria-label*='2']\")\n",
    "        clicked = False\n",
    "        for el in candidates:\n",
    "            try:\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", el)\n",
    "                time.sleep(0.2)\n",
    "                el.click()\n",
    "                time.sleep(1.2)\n",
    "                clicked = True\n",
    "                break\n",
    "            except Exception:\n",
    "                continue\n",
    "        if not clicked:\n",
    "            # fallback next\n",
    "            for sel in [\"a[aria-label*='Next']\", \"button[aria-label*='Next']\",\n",
    "                        \"li.sg-pagination__next a\", \"button.sg-pagination__next\"]:\n",
    "                try:\n",
    "                    el = nav.find_element(By.CSS_SELECTOR, sel)\n",
    "                    driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", el)\n",
    "                    time.sleep(0.2)\n",
    "                    el.click()\n",
    "                    time.sleep(1.2)\n",
    "                    clicked = True\n",
    "                    break\n",
    "                except Exception:\n",
    "                    continue\n",
    "        if clicked:\n",
    "            items.extend(read_cards())\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return items\n",
    "\n",
    "open_listing_and_filter()\n",
    "programmes = collect_programmes_two_pages()\n",
    "print(\"Programmes found:\", len(programmes))\n",
    "[itm[\"title\"] for itm in programmes]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c734ce",
   "metadata": {},
   "source": [
    "## 4. Studiegids parsers all tracks and years\n",
    "\n",
    "Opens the curriculum tab, expands every track, then every year.\n",
    "Skips sections labeled Minor and reads only visible tables.\n",
    "Extracts course name, period, ECTS, code, track, and year label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f70cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_tab_three(url):\n",
    "    base = url.split(\"#/tab=\")[0]\n",
    "    driver.get(f\"{base}#/tab=3\")\n",
    "    dismiss_cookies()\n",
    "    time.sleep(0.8)\n",
    "\n",
    "def list_tracks():\n",
    "    items = driver.find_elements(By.CSS_SELECTOR, \"#study-program .accordion > div\")\n",
    "    out = []\n",
    "    for tr in items:\n",
    "        label = safe_text(tr)\n",
    "        if is_minor_text(label):\n",
    "            continue\n",
    "        out.append(tr)\n",
    "    return out\n",
    "\n",
    "def expand_if_collapsed(container):\n",
    "    try:\n",
    "        content = container.find_element(By.CSS_SELECTOR, \".accordion-content\")\n",
    "    except Exception:\n",
    "        content = None\n",
    "\n",
    "    if content and content.is_displayed():\n",
    "        return\n",
    "\n",
    "    for sel in [\"button\", \".accordion-title\"]:\n",
    "        try:\n",
    "            btn = container.find_element(By.CSS_SELECTOR, sel)\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", btn)\n",
    "            time.sleep(0.2)\n",
    "            btn.click()\n",
    "            time.sleep(0.8)\n",
    "            break\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "\n",
    "def list_year_items(track_container):\n",
    "    try:\n",
    "        items = track_container.find_elements(By.CSS_SELECTOR, \".accordion .accordion-item, .accordion > div\")\n",
    "    except Exception:\n",
    "        return []\n",
    "    out = []\n",
    "    for it in items:\n",
    "        yl = safe_text(it)\n",
    "        if is_minor_text(yl):\n",
    "            continue\n",
    "        out.append(it)\n",
    "    return out\n",
    "\n",
    "def parse_visible_tables(scope_container):\n",
    "    # read all visible tables under the given container\n",
    "    out = []\n",
    "    contents = scope_container.find_elements(By.CSS_SELECTOR, \".accordion-content\")\n",
    "    vis = [c for c in contents if c.is_displayed()]\n",
    "    if not vis and contents:\n",
    "        vis = [contents[0]]\n",
    "\n",
    "    for cont in vis:\n",
    "        tables = cont.find_elements(By.CSS_SELECTOR, \"table tbody\")\n",
    "        for tb in tables:\n",
    "            if not tb.is_displayed():\n",
    "                continue\n",
    "            rows = tb.find_elements(By.CSS_SELECTOR, \"tr\")\n",
    "            for r in rows:\n",
    "                tds = r.find_elements(By.CSS_SELECTOR, \"td\")\n",
    "                if not tds:\n",
    "                    continue\n",
    "                # name\n",
    "                try:\n",
    "                    name = tds[0].find_element(By.CSS_SELECTOR, \"a\").text.strip()\n",
    "                except Exception:\n",
    "                    name = tds[0].text.strip()\n",
    "                # period\n",
    "                per = None\n",
    "                if len(tds) > 1:\n",
    "                    try:\n",
    "                        per_text = tds[1].find_element(By.CSS_SELECTOR, \"a\").text.strip()\n",
    "                    except Exception:\n",
    "                        per_text = tds[1].text.strip()\n",
    "                    m = re.search(r\"(\\d+)\", per_text)\n",
    "                    per = int(m.group(1)) if m else None\n",
    "                # ects\n",
    "                ects = None\n",
    "                if len(tds) > 2:\n",
    "                    try:\n",
    "                        ects_text = tds[2].find_element(By.CSS_SELECTOR, \"a\").text.strip()\n",
    "                    except Exception:\n",
    "                        ects_text = tds[2].text.strip()\n",
    "                    m = re.search(r\"(\\d+)\", ects_text)\n",
    "                    ects = int(m.group(1)) if m else None\n",
    "                # code\n",
    "                code = \"\"\n",
    "                if len(tds) > 3:\n",
    "                    try:\n",
    "                        code = tds[3].find_element(By.CSS_SELECTOR, \"a\").text.strip()\n",
    "                    except Exception:\n",
    "                        code = tds[3].text.strip()\n",
    "\n",
    "                if name:\n",
    "                    out.append({\"course_name\": name, \"period\": per, \"ects\": ects, \"code\": code})\n",
    "    return out\n",
    "\n",
    "def parse_studiegids_all_tracks_years(url):\n",
    "    open_tab_three(url)\n",
    "\n",
    "    out = []\n",
    "    visited = set()  # guards against double parsing if DOM reflows\n",
    "\n",
    "    tracks = list_tracks()\n",
    "    for tr in tracks:\n",
    "        tr_label = safe_text(tr)\n",
    "        if is_minor_text(tr_label):\n",
    "            continue\n",
    "\n",
    "        expand_if_collapsed(tr)\n",
    "\n",
    "        year_items = list_year_items(tr)\n",
    "        if not year_items:\n",
    "            # table directly under track\n",
    "            rows = parse_visible_tables(tr)\n",
    "            for row in rows:\n",
    "                key = (tr_label, row.get(\"course_name\"), row.get(\"code\"))\n",
    "                if key in visited:\n",
    "                    continue\n",
    "                visited.add(key)\n",
    "                row[\"track\"] = tr_label\n",
    "                row[\"year_label\"] = \"\"\n",
    "                out.append(row)\n",
    "            continue\n",
    "\n",
    "        for yi in year_items:\n",
    "            yl = safe_text(yi)\n",
    "            if is_minor_text(yl):\n",
    "                continue\n",
    "\n",
    "            expand_if_collapsed(yi)\n",
    "            rows = parse_visible_tables(yi)\n",
    "            for row in rows:\n",
    "                key = (tr_label, yl, row.get(\"course_name\"), row.get(\"code\"))\n",
    "                if key in visited:\n",
    "                    continue\n",
    "                visited.add(key)\n",
    "                row[\"track\"] = tr_label\n",
    "                row[\"year_label\"] = yl\n",
    "                out.append(row)\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032e0cb9",
   "metadata": {},
   "source": [
    "## 5. vu.nl helpers with cache\n",
    "\n",
    "Finds the base vu.nl URL from studiegids info links.\n",
    "Then it fetches base, curriculum, future, and admissions pages with caching.\n",
    "Parses descriptions and optional curriculum cards as fallback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da73758",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = DATA_DIR / \"vu_cache\"\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def cache_path(url):\n",
    "    slug = re.sub(r\"[^a-zA-Z0-9]+\", \"_\", url.strip(\"/\"))[:180]\n",
    "    return CACHE_DIR / f\"{slug}.html\"\n",
    "\n",
    "#def static_soup(url):\n",
    "    fp = cache_path(url)\n",
    "    if fp.exists():\n",
    "        html = fp.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        return BeautifulSoup(html, \"lxml\")\n",
    "    r = requests.get(url, headers=HEADERS, timeout=25)\n",
    "    r.raise_for_status()\n",
    "    fp.write_text(r.text, encoding=\"utf-8\")\n",
    "    return BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "def static_soup(url):\n",
    "    fp = cache_path(url)\n",
    "    if fp.exists():\n",
    "        html = fp.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        return BeautifulSoup(html, \"lxml\")\n",
    "    r = requests.get(url, headers=HEADERS, timeout=25)\n",
    "    r.raise_for_status()\n",
    "    fp.write_text(r.text, encoding=\"utf-8\")\n",
    "    return BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "def pick_vu_base(info_links):\n",
    "    for u in info_links:\n",
    "        if not u:\n",
    "            continue\n",
    "        if \"vu.nl\" in u and \"/education/bachelor/\" in u:\n",
    "            base = u.split(\"?\")[0].rstrip(\"/\")\n",
    "            for tail in [\"/curriculum\", \"/future\", \"/admissions\", \"/careers\"]:\n",
    "                if base.endswith(tail):\n",
    "                    base = base[: -len(tail)]\n",
    "            return base\n",
    "    return None\n",
    "\n",
    "def parse_vu_base(base_url):\n",
    "    soup = static_soup(base_url)\n",
    "    if soup is None:\n",
    "        return {\"vunl_description\": \"\"}\n",
    "    el = soup.select_one(\".xlarge-offset-0\")\n",
    "    desc = el.get_text(strip=True) if el else \"\"\n",
    "    return {\"vunl_description\": desc}\n",
    "\n",
    "def parse_vu_curriculum(base_url):\n",
    "    url, soup = discover_section_url(base_url, \"curriculum\")\n",
    "    if soup is None:\n",
    "        return {\"vunl_description_curriculum\": \"\", \"vunl_firstyear_description_blocks\": [], \"vunl_subjects\": []}\n",
    "\n",
    "    desc_el = soup.select_one(\".xlarge-offset-0\")\n",
    "    desc = desc_el.get_text(strip=True) if desc_el else \"\"\n",
    "    year_blocks = [b.get_text(strip=True) for b in soup.select(\".xxlarge-6\")] or []\n",
    "\n",
    "    subjects = []\n",
    "    for card in soup.select(\".vuw-card-border-left\"):\n",
    "        text = card.get_text(\"\\n\", strip=True)\n",
    "        name = text.split(\"\\n\")[0].strip()\n",
    "        # skip minors\n",
    "        if re.search(r\"\\bminor\\b\", name, flags=re.I):\n",
    "            continue\n",
    "        ects = None\n",
    "        period = None\n",
    "        year = None\n",
    "        m = re.search(r\"(\\d+)\\s*ECTS\", text, flags=re.I)\n",
    "        if m:\n",
    "            ects = int(m.group(1))\n",
    "        m = re.search(r\"Period\\s*([0-9]+)\", text, flags=re.I)\n",
    "        if m:\n",
    "            period = int(m.group(1))\n",
    "        m = re.search(r\"Year\\s*([0-9]+)\", text, flags=re.I)\n",
    "        if m:\n",
    "            year = int(m.group(1))\n",
    "        subjects.append({\"course_name\": name, \"ects\": ects, \"period\": period, \"year\": year})\n",
    "\n",
    "    return {\n",
    "        \"vunl_description_curriculum\": desc,\n",
    "        \"vunl_firstyear_description_blocks\": year_blocks,\n",
    "        \"vunl_subjects\": subjects\n",
    "    }\n",
    "\n",
    "def parse_vu_future(base_url):\n",
    "    url, soup = discover_section_url(base_url, \"future\")\n",
    "    if soup is None:\n",
    "        return {\"vunl_future_description\": \"\", \"vunl_future_career\": \"\"}\n",
    "\n",
    "    desc_el = soup.select_one(\".xlarge-offset-0\")\n",
    "    desc = desc_el.get_text(strip=True) if desc_el else \"\"\n",
    "\n",
    "    career_el = soup.select_one(\".large-6 + .large-6 .vuw-p-6\")\n",
    "    career = career_el.get_text(\" \", strip=True) if career_el else \"\"\n",
    "    return {\"vunl_future_description\": desc, \"vunl_future_career\": career}\n",
    "\n",
    "def parse_vu_admissions(base_url):\n",
    "    url, soup = discover_section_url(base_url, \"admissions\")\n",
    "    if soup is None:\n",
    "        return {\"vunl_admission_dutch_diploma\": \"\"}\n",
    "\n",
    "    rich = [el.get_text(\" \", strip=True) for el in soup.select(\".vuw-rich-text\")]\n",
    "    dutch = \"\"\n",
    "    for t in rich:\n",
    "        if \"Dutch\" in t or \"VWO\" in t or \"Dutch diploma\" in t:\n",
    "            dutch = t\n",
    "            break\n",
    "    return {\"vunl_admission_dutch_diploma\": dutch}\n",
    "\n",
    "\n",
    "# find the best matching section url on the base page\n",
    "def discover_section_url(base_url, want):\n",
    "    \"\"\"\n",
    "    want can be 'curriculum', 'future', or 'admissions'\n",
    "    Try common slugs. If those 404, scan the base page for anchors\n",
    "    whose text or href matches the idea.\n",
    "    \"\"\"\n",
    "    slug_map = {\n",
    "        \"curriculum\": [\"curriculum\", \"study-programme\", \"programme\", \"program\"],\n",
    "        \"future\": [\"future\", \"your-future-career\", \"career\"],\n",
    "        \"admissions\": [\"admissions\", \"admission\", \"how-to-apply\", \"apply\"]\n",
    "    }\n",
    "    # try common slugs first\n",
    "    for slug in slug_map.get(want, []):\n",
    "        url = base_url.rstrip(\"/\") + \"/\" + slug\n",
    "        soup = static_soup(url)\n",
    "        if soup is not None:\n",
    "            return url, soup\n",
    "\n",
    "    # fall back to scanning anchors on the base page\n",
    "    base_soup = static_soup(base_url)\n",
    "    if base_soup is None:\n",
    "        return None, None\n",
    "\n",
    "    want_words = {\n",
    "        \"curriculum\": [\"curriculum\", \"study programme\", \"courses\"],\n",
    "        \"future\": [\"future\", \"career\", \"after graduation\"],\n",
    "        \"admissions\": [\"admissions\", \"admission\", \"apply\"]\n",
    "    }[want]\n",
    "\n",
    "    for a in base_soup.select(\"a[href]\"):\n",
    "        txt = a.get_text(\" \", strip=True).lower()\n",
    "        href = a.get(\"href\", \"\")\n",
    "        href_l = href.lower()\n",
    "        if any(w in txt for w in want_words) or any(w in href_l for w in want_words):\n",
    "            # build absolute url if needed\n",
    "            if href.startswith(\"http\"):\n",
    "                u = href\n",
    "            else:\n",
    "                # base_url is absolute, join relative path\n",
    "                u = base_url.rstrip(\"/\") + \"/\" + href.lstrip(\"/\")\n",
    "            soup = static_soup(u)\n",
    "            if soup is not None:\n",
    "                return u, soup\n",
    "\n",
    "    return None, None\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4087827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a smaller sample of the data\n",
    "#programmes = programmes[12:13]  # limit to first 2 for testing\n",
    "\n",
    "# generate a smaller sample of the data\n",
    "TEST_TITLES = {\n",
    "    # \"Archaeology\",\n",
    "    \"Artificial Intelligence\"\n",
    "    #,\n",
    "#    \"Philosophy, Politics and Economics\",\n",
    " #   \"Computer Science\"\n",
    "}\n",
    "# filter programmes to include only those in TEST_TITLES\n",
    "programmes = [p for p in programmes if p[\"title\"] in TEST_TITLES]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5513d6",
   "metadata": {},
   "source": [
    "## 6. Main pipeline scrape and enrich\n",
    "\n",
    "Loops over programmes, captures studiegids description and info links.\n",
    "Then, it builds tidy rows for programmes and subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e22d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_prog = []\n",
    "rows_subj = []\n",
    "\n",
    "for item in tqdm(programmes, desc=\"Programmes\"):\n",
    "    url = item[\"url\"]\n",
    "    title = item[\"title\"]\n",
    "\n",
    "    prog = {\"programme_title\": title, \"programme_url\": url}\n",
    "\n",
    "    # studiegids tab two for description and info links\n",
    "    base = url.split(\"#/tab=\")[0]\n",
    "    driver.get(f\"{base}#/tab=2\")\n",
    "    dismiss_cookies()\n",
    "    time.sleep(0.8)\n",
    "    try:\n",
    "        prog[\"sg_description\"] = driver.find_element(By.CSS_SELECTOR, \"#study-description\").text.strip()\n",
    "    except Exception:\n",
    "        prog[\"sg_description\"] = \"\"\n",
    "    info_links = []\n",
    "    try:\n",
    "        info_block = driver.find_element(By.CSS_SELECTOR, \".info\")\n",
    "        anchors = info_block.find_elements(By.CSS_SELECTOR, \"a[href]\")\n",
    "        info_links = [a.get_attribute(\"href\") for a in anchors]\n",
    "    except Exception:\n",
    "        pass\n",
    "    prog[\"info_links\"] = info_links\n",
    "\n",
    "    # subjects from all tracks and years\n",
    "    all_rows = parse_studiegids_all_tracks_years(url)\n",
    "    for r in all_rows:\n",
    "        rows_subj.append({\n",
    "            \"programme_title\": title,\n",
    "            \"programme_url\": url,\n",
    "            \"track\": r.get(\"track\", \"\"),\n",
    "            \"year_label\": r.get(\"year_label\", \"\"),\n",
    "            \"course_name\": r.get(\"course_name\"),\n",
    "            \"period\": r.get(\"period\"),\n",
    "            \"ects\": r.get(\"ects\"),\n",
    "            \"code\": r.get(\"code\")\n",
    "        })\n",
    "\n",
    "    # vu.nl enrichment\n",
    "    vu_base = pick_vu_base(info_links)\n",
    "    prog[\"vunl_base_url\"] = vu_base\n",
    "    if vu_base:\n",
    "        try:\n",
    "            prog.update(parse_vu_base(vu_base))\n",
    "        except Exception:\n",
    "            prog[\"vunl_description\"] = \"\"\n",
    "        try:\n",
    "            cur = parse_vu_curriculum(vu_base)\n",
    "            prog.update({k: v for k, v in cur.items() if k != \"vunl_subjects\"})\n",
    "        except Exception:\n",
    "            prog[\"vunl_description_curriculum\"] = \"\"\n",
    "            prog[\"vunl_firstyear_description_blocks\"] = []\n",
    "        try:\n",
    "            fut = parse_vu_future(vu_base)\n",
    "            prog.update(fut)\n",
    "        except Exception:\n",
    "            prog[\"vunl_future_description\"] = \"\"\n",
    "            prog[\"vunl_future_career\"] = \"\"\n",
    "        try:\n",
    "            prog.update(parse_vu_admissions(vu_base))\n",
    "        except Exception:\n",
    "            prog[\"vunl_admission_dutch_diploma\"] = \"\"\n",
    "\n",
    "    rows_prog.append(prog)\n",
    "\n",
    "df_prog = pd.DataFrame(rows_prog).drop_duplicates(subset=[\"programme_url\"]).reset_index(drop=True)\n",
    "df_subj = pd.DataFrame(rows_subj).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "print(\"Programmes shape\", df_prog.shape)\n",
    "print(\"Subjects shape\", df_subj.shape)\n",
    "\n",
    "df_prog.head(2), df_subj.head(10)\n",
    "\n",
    "#time: 44.50 min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1a0bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save programmes with pdf info\n",
    "df_prog.to_csv(DATA_DIR / \"df_programmes_bronze.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "df_prog.to_json(DATA_DIR / \"df_programmes_bronze.json\", orient=\"records\", force_ascii=False, indent=2)\n",
    "df_subj.to_csv(DATA_DIR / \"df_subj_temp_bronze.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420e625f",
   "metadata": {},
   "source": [
    "## 7. Clean df_subj \n",
    "Deduplicate and generates other columns: Year and Track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13499811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows where the track column contains the word Year\n",
    "#mask_keep = ~df_subj[\"track\"].fillna(\"\").str.contains(r\"\\byear\\b\", case=False, regex=True)\n",
    "#df_subj = df_subj[mask_keep].copy()\n",
    "\n",
    "# join text columns track and year_label into parsed columns called track1\n",
    "df_subj['track1'] = df_subj[\"track\"].fillna(\"\") + \" \" + df_subj[\"year_label\"].fillna(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cab9e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# parser that extracts Track name and Year number from year_label\n",
    "def parse_track_year(label):\n",
    "    # expected shape like: Bachelor Ancient Studies, Track Ancient Studies Year 1\n",
    "    # or: Bachelor Ancient Studies, Specialization Archaeology Year 3\n",
    "    if not isinstance(label, str) or not label.strip():\n",
    "        return pd.Series([None, None])\n",
    "    m = re.search(r\"(?:Track|Specialization)\\s+(.+?)\\s+Year\\s+(\\d+)\", label, flags=re.I)\n",
    "    if m:\n",
    "        track_name = m.group(1).strip()\n",
    "        year_num = int(m.group(2))\n",
    "        return pd.Series([track_name, year_num])\n",
    "    # fallback: find the part before Year and strip a leading Track or Specialization\n",
    "    m2 = re.search(r\"Year\\s+(\\d+)\", label, flags=re.I)\n",
    "    if m2:\n",
    "        year_num = int(m2.group(1))\n",
    "        before = label[:m2.start()]\n",
    "        tail = before.split(\",\")[-1].strip()\n",
    "        tail = re.sub(r\"^(Track|Specialization)\\s+\", \"\", tail, flags=re.I).strip()\n",
    "        track_name = tail if tail else None\n",
    "        return pd.Series([track_name, year_num])\n",
    "    return pd.Series([None, None])\n",
    "\n",
    "df_subj[[\"track_from_label\", \"year_num\"]] = df_subj[\"track1\"].apply(parse_track_year)\n",
    "\n",
    "# optional: fill missing track_from_label with the existing track text\n",
    "df_subj[\"track_from_label\"] = df_subj[\"track_from_label\"].fillna(df_subj[\"track\"])\n",
    "\n",
    "# ensure numeric types where possible\n",
    "df_subj[\"year_num\"] = pd.to_numeric(df_subj[\"year_num\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# quick check\n",
    "df_subj[[\"programme_title\", \"track\", \"year_label\", \"track_from_label\", \"year_num\"]].head(8)\n",
    "\n",
    "# remove columns no longer needed\n",
    "df_subj = df_subj.drop(columns=[\"track\", \"year_label\", \"track1\", \"track_from_label\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d3ed1b",
   "metadata": {},
   "source": [
    "## 8. Download programme PDFs from info_links into data/bachelor_programs_pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5777fc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download first pdf link from info_links for each programme\n",
    "import re, ast, requests\n",
    "from urllib.parse import urlparse\n",
    "PDF_DIR = DATA_DIR / \"bachelor_programs_pdfs\"\n",
    "PDF_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def sanitize_filename(s):\n",
    "    # remove characters that are unsafe for filenames\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", s).strip()\n",
    "\n",
    "def first_pdf_url(links):\n",
    "    # links can be a list or a string representation of a list\n",
    "    if links is None:\n",
    "        return None\n",
    "    if isinstance(links, str):\n",
    "        # try parse as a list representation\n",
    "        try:\n",
    "            maybe_list = ast.literal_eval(links)\n",
    "            if isinstance(maybe_list, list):\n",
    "                links = maybe_list\n",
    "            else:\n",
    "                links = [links]\n",
    "        except Exception:\n",
    "            links = [links]\n",
    "    if not isinstance(links, list):\n",
    "        return None\n",
    "    for u in links:\n",
    "        if not isinstance(u, str):\n",
    "            continue\n",
    "        path = urlparse(u).path.lower()\n",
    "        if path.endswith(\".pdf\"):\n",
    "            return u\n",
    "    return None\n",
    "\n",
    "# pick the first pdf url per programme\n",
    "df_prog[\"pdf_url\"] = df_prog[\"info_links\"].apply(first_pdf_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4692501",
   "metadata": {},
   "source": [
    "## 9. Scrape data from courses pages\n",
    "\n",
    "### Build df_courses seed from df_subj\n",
    "Creates a unique list of course codes and a resolvable course URL per code.\n",
    "Uses the first programme URL that contains the code to build the course URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16481652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build df_courses with unique course codes and a resolvable course URL\n",
    "# keep non empty codes\n",
    "codes = (\n",
    "    df_subj[[\"programme_url\",\"code\"]]\n",
    "    .dropna(subset=[\"code\"])\n",
    "    .query(\"code.str.strip() != ''\", engine=\"python\")\n",
    "    .drop_duplicates(subset=[\"code\"])\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "# construct a course URL using the first programme base for that code\n",
    "def make_course_url(row):\n",
    "    base = row[\"programme_url\"].split(\"#\")[0].rstrip(\"/\")\n",
    "    return f\"{base}/{row['code']}#/\"\n",
    "codes[\"course_url\"] = codes.apply(make_course_url, axis=1)\n",
    "\n",
    "df_courses = codes[[\"code\",\"course_url\"]].reset_index(drop=True)\n",
    "#df_courses.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b3bec8",
   "metadata": {},
   "source": [
    "## 10. Course scraper helpers\n",
    "\n",
    "Adds safe text helpers and robust table readers.\n",
    "Expands any dropdowns, then collects programme names\n",
    "Provides a generic label finder inside any table and a pdf finder that scans the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6429a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers for safe text, label search, and pdf discovery\n",
    "import re, time, json, requests, os\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "COURSE_PDF_DIR = DATA_DIR / \"bachelor_courses_pdf\"\n",
    "COURSE_PDF_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def txt(el):\n",
    "    try:\n",
    "        return el.text.strip()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def first_text_or_blank(els):\n",
    "    return txt(els[0]) if els else \"\"\n",
    "\n",
    "def wait_for_course_dom():\n",
    "    # wait for either a table or a paragraph block to be present\n",
    "    WebDriverWait(driver, 20).until(\n",
    "        lambda d: d.find_elements(By.CSS_SELECTOR, \"table\") or d.find_elements(By.CSS_SELECTOR, \".paragraph\")\n",
    "    )\n",
    "\n",
    "def tables_on_page():\n",
    "    return driver.find_elements(By.CSS_SELECTOR, \"table\")\n",
    "\n",
    "def find_value_by_label(labels):\n",
    "    \"\"\"\n",
    "    Search all tables for a row that contains any of the given labels.\n",
    "    Return the text from the opposite cell or the last cell if there are many.\n",
    "    \"\"\"\n",
    "    lab_norm = [l.lower() for l in labels]\n",
    "    for tbl in tables_on_page():\n",
    "        rows = tbl.find_elements(By.CSS_SELECTOR, \"tr\")\n",
    "        for r in rows:\n",
    "            cells = r.find_elements(By.CSS_SELECTOR, \"td, th\")\n",
    "            if not cells:\n",
    "                continue\n",
    "            cell_texts = [txt(c) for c in cells]\n",
    "            joined = \" | \".join(cell_texts).lower()\n",
    "            if any(l in joined for l in lab_norm):\n",
    "                if len(cells) >= 2:\n",
    "                    # assume last cell carries the value\n",
    "                    return txt(cells[-1])\n",
    "                else:\n",
    "                    return joined\n",
    "    return \"\"\n",
    "\n",
    "def teaching_method_from_context():\n",
    "    \"\"\"\n",
    "    Try nearest table that follows a paragraph titled Teaching method.\n",
    "    Fallback to label based lookup inside any table.\n",
    "    \"\"\"\n",
    "    # nearby table after a paragraph\n",
    "    try:\n",
    "        para = driver.find_element(By.XPATH, \"//p[contains(., 'Teaching method')]\")\n",
    "        tbl = para.find_element(By.XPATH, \"following-sibling::table[1]\")\n",
    "        cells = [txt(td) for td in tbl.find_elements(By.CSS_SELECTOR, \"td\")]\n",
    "        cells = [c for c in cells if c]\n",
    "        if cells:\n",
    "            return \"; \".join(cells)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # fallback to label search inside tables\n",
    "    return find_value_by_label([\"Teaching method\", \"Teaching methods\"])\n",
    "\n",
    "def click_all_dropdowns():\n",
    "    # expand any dropdown sections that list programme memberships\n",
    "    headers = driver.find_elements(By.CSS_SELECTOR, \".dropdown-header\")\n",
    "    for h in headers:\n",
    "        try:\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", h)\n",
    "            time.sleep(0.2)\n",
    "            h.click()\n",
    "            time.sleep(0.4)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "def list_programmes_from_dropdown():\n",
    "    # gather programme names from expanded sections\n",
    "    names = []\n",
    "    for sec in driver.find_elements(By.CSS_SELECTOR, \".dropdown-section\"):\n",
    "        for el in sec.find_elements(By.CSS_SELECTOR, \"a, li, p, span\"):\n",
    "            t = txt(el)\n",
    "            if t and len(t) > 2:\n",
    "                names.append(t)\n",
    "    # unique preserve order\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for n in names:\n",
    "        if n not in seen:\n",
    "            seen.add(n)\n",
    "            out.append(n)\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680fb2f5",
   "metadata": {},
   "source": [
    "## 11. Parse a single course page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9790a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_course_page(course_url, code):\n",
    "    # open and wait\n",
    "    driver.get(course_url)\n",
    "    dismiss_cookies()\n",
    "    wait_for_course_dom()\n",
    "\n",
    "    # course level and coordinator by label search\n",
    "    level = find_value_by_label([\"Course level\", \"Level\"])\n",
    "    coordinator = find_value_by_label([\"Course coordinator\", \"Coordinator\"])\n",
    "\n",
    "    # faculty by label, then fallback to table row position\n",
    "    faculty = find_value_by_label([\"Faculty\", \"Department\", \"School\"])\n",
    "    if not faculty:\n",
    "        try:\n",
    "            # fallback exact position\n",
    "            faculty = driver.find_element(By.CSS_SELECTOR, \"tr:nth-child(6) td:last-child\").text.strip()\n",
    "        except Exception:\n",
    "            faculty = \"\"\n",
    "\n",
    "    # teaching method with nearby context or label\n",
    "    teaching_method = teaching_method_from_context()\n",
    "\n",
    "    # expand dropdowns and collect programme memberships\n",
    "    click_all_dropdowns()\n",
    "    course_programmes = \"; \".join(list_programmes_from_dropdown())\n",
    "\n",
    "    # paragraphs block\n",
    "    paragraphs = [txt(p) for p in driver.find_elements(By.CSS_SELECTOR, \".paragraph\") if txt(p)]\n",
    "    paragraphs_json = json.dumps(paragraphs, ensure_ascii=False)\n",
    "\n",
    "    # final fallbacks if still blank\n",
    "    if not level:\n",
    "        try:\n",
    "            level = driver.find_element(By.CSS_SELECTOR, \"tr:nth-child(4) td:last-child\").text.strip()\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not coordinator:\n",
    "        try:\n",
    "            coordinator = driver.find_element(By.CSS_SELECTOR, \"tr:nth-child(7) td:last-child\").text.strip()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return {\n",
    "        \"code\": code,\n",
    "        \"faculty\": faculty,\n",
    "        \"course_level\": level,\n",
    "        \"course_coordinator\": coordinator,\n",
    "        \"teaching_method\": teaching_method,\n",
    "        \"course_programmes\": course_programmes,\n",
    "        \"course_paragraphs_json\": paragraphs_json\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677ee852",
   "metadata": {},
   "source": [
    "## 12. Run the course scrape and build df_courses_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a81b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce df_courses for testing\n",
    "#df_courses = df_courses.head(3).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfcc36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "rows = []\n",
    "for _, row in tqdm(df_courses.iterrows(), total=len(df_courses), desc=\"Courses\"):\n",
    "    url = row[\"course_url\"]\n",
    "    code = row[\"code\"]\n",
    "    try:\n",
    "        data = parse_course_page(url, code)\n",
    "        rows.append(data)\n",
    "    except Exception as e:\n",
    "        rows.append({\n",
    "            \"code\": code,\n",
    "            \"faculty\": \"\",\n",
    "            \"course_level\": \"\",\n",
    "            \"course_coordinator\": \"\",\n",
    "            \"teaching_method\": \"\",\n",
    "            \"course_programmes\": \"\",\n",
    "            \"course_paragraphs_json\": \"[]\"\n",
    "        })\n",
    "\n",
    "df_courses_full = pd.DataFrame(rows).drop_duplicates(subset=[\"code\"]).reset_index(drop=True)\n",
    "print(df_courses_full.head())\n",
    "\n",
    "# 87 min run time for 420 courses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d413e2",
   "metadata": {},
   "source": [
    "## 13. Left join and ordering of the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b93a312",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy = df_courses_full.copy()\n",
    "\n",
    "print(df_courses_full.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4337228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# left join subject data with course details on code\n",
    "df_subj_full = df_subj.merge(df_courses_full, on=\"code\", how=\"right\")\n",
    "\n",
    "# change order of columns for clarity\n",
    "cols_order = ['code', 'course_name', 'programme_title', 'faculty', 'programme_url',\n",
    "               'year_num', 'period', 'ects', 'course_level',\n",
    "              'course_coordinator', 'teaching_method', 'course_programmes',\n",
    "              'course_paragraphs_json']\n",
    "df_subj_full = df_subj_full[cols_order]\n",
    "\n",
    "df_subj_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3100da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save for reuse\n",
    "df_subj_full.to_csv(DATA_DIR / \"df_courses_bronze.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "df_subj_full.to_json(DATA_DIR / \"df_courses_bronze.json\", orient=\"records\", force_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
